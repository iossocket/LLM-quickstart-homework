{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p2RAEZXs-8T",
        "outputId": "7de16fc0-51a5-41b2-c4fa-09f36fbc0aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ChatGLM3' already exists and is not an empty directory.\n",
            "/content/ChatGLM3/finetune_demo\n",
            "Requirement already satisfied: jieba>=0.42.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.42.1)\n",
            "Requirement already satisfied: ruamel_yaml>=0.18.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.18.6)\n",
            "Requirement already satisfied: rouge_chinese>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.0.3)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: datasets>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.18.0)\n",
            "Requirement already satisfied: peft>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: transformers>=4.38.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.38.2)\n",
            "Requirement already satisfied: deepspeed==0.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.13.1)\n",
            "Requirement already satisfied: mpi4py>=3.1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (3.1.5)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (1.11.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (2.6.4)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (11.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.1->-r requirements.txt (line 8)) (4.66.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel_yaml>=0.18.6->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge_chinese>=1.0.3->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (5.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (7.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.17.1->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (0.28.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (0.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.1->-r requirements.txt (line 7)) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.1->-r requirements.txt (line 7)) (0.15.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.17.1->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.17.1->-r requirements.txt (line 5)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.17.1->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.17.1->-r requirements.txt (line 5)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.17.1->-r requirements.txt (line 5)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.17.1->-r requirements.txt (line 5)) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets>=2.17.1->-r requirements.txt (line 5)) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.17.1->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.17.1->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.17.1->-r requirements.txt (line 5)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.17.1->-r requirements.txt (line 5)) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (12.4.99)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.10)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.16.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.10.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.17.1->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.17.1->-r requirements.txt (line 5)) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 8)) (2.16.3)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.2.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.19.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.5.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.13.1->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.8.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.18.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM3.git\n",
        "%cd ChatGLM3/finetune_demo\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/AdvertiseGen_fix/"
      ],
      "metadata": {
        "id": "pXxCrE44awPy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Union\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _resolve_path(path: Union[str, Path]) -> Path:\n",
        "    return Path(path).expanduser().resolve()\n",
        "\n",
        "\n",
        "def _mkdir(dir_name: Union[str, Path]):\n",
        "    dir_name = _resolve_path(dir_name)\n",
        "    if not dir_name.is_dir():\n",
        "        dir_name.mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "\n",
        "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
        "    def _convert(in_file: Path, out_file: Path):\n",
        "        _mkdir(out_file.parent)\n",
        "        with open(in_file, encoding='utf-8') as fin:\n",
        "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
        "                for line in fin:\n",
        "                    dct = json.loads(line)\n",
        "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
        "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
        "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    data_dir = _resolve_path(data_dir)\n",
        "    save_dir = _resolve_path(save_dir)\n",
        "\n",
        "    train_file = data_dir / 'train.json'\n",
        "    if train_file.is_file():\n",
        "        out_file = save_dir / train_file.relative_to(data_dir)\n",
        "        _convert(train_file, out_file)\n",
        "\n",
        "    dev_file = data_dir / 'dev.json'\n",
        "    if dev_file.is_file():\n",
        "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
        "        _convert(dev_file, out_file)\n",
        "\n",
        "\n",
        "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
      ],
      "metadata": {
        "id": "Cfg-Nzoyt1Xo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune_hf.py data/AdvertiseGen_fix THUDM/chatglm3-6b configs/lora.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT6Wc_zKt_7K",
        "outputId": "efb57a2c-c43f-457b-c270-b07f31e9b4ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-01 06:55:21.480220: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-01 06:55:21.480269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-01 06:55:21.482302: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-01 06:55:22.541993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Setting eos_token is not supported, use the default one.\n",
            "Setting pad_token is not supported, use the default one.\n",
            "Setting unk_token is not supported, use the default one.\n",
            "Loading checkpoint shards: 100% 7/7 [00:03<00:00,  2.08it/s]\n",
            "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
            "--> Model\n",
            "\n",
            "--> model has 1.949696M params\n",
            "\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 114599 examples [00:00, 609413.92 examples/s]\n",
            "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
            "Generating validation split: 1070 examples [00:00, 266044.54 examples/s]\n",
            "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
            "Generating test split: 1070 examples [00:00, 305882.31 examples/s]\n",
            "Map (num_proc=16): 100% 114599/114599 [00:04<00:00, 26850.41 examples/s]\n",
            "train_dataset: Dataset({\n",
            "    features: ['input_ids', 'labels'],\n",
            "    num_rows: 114599\n",
            "})\n",
            "Map (num_proc=16): 100% 1070/1070 [00:00<00:00, 1752.16 examples/s]\n",
            "val_dataset: Dataset({\n",
            "    features: ['input_ids', 'output_ids'],\n",
            "    num_rows: 1070\n",
            "})\n",
            "Map (num_proc=16): 100% 1070/1070 [00:00<00:00, 1694.23 examples/s]\n",
            "test_dataset: Dataset({\n",
            "    features: ['input_ids', 'output_ids'],\n",
            "    num_rows: 1070\n",
            "})\n",
            "--> Sanity check\n",
            "           '[gMASK]': 64790 -> -100\n",
            "               'sop': 64792 -> -100\n",
            "          '<|user|>': 64795 -> -100\n",
            "                  '': 30910 -> -100\n",
            "                '\\n': 13 -> -100\n",
            "                  '': 30910 -> -100\n",
            "                '类型': 33467 -> -100\n",
            "                 '#': 31010 -> -100\n",
            "                 '裤': 56532 -> -100\n",
            "                 '*': 30998 -> -100\n",
            "                 '版': 55090 -> -100\n",
            "                 '型': 54888 -> -100\n",
            "                 '#': 31010 -> -100\n",
            "                '宽松': 40833 -> -100\n",
            "                 '*': 30998 -> -100\n",
            "                '风格': 32799 -> -100\n",
            "                 '#': 31010 -> -100\n",
            "                '性感': 40589 -> -100\n",
            "                 '*': 30998 -> -100\n",
            "                '图案': 37505 -> -100\n",
            "                 '#': 31010 -> -100\n",
            "                '线条': 37216 -> -100\n",
            "                 '*': 30998 -> -100\n",
            "                 '裤': 56532 -> -100\n",
            "                 '型': 54888 -> -100\n",
            "                 '#': 31010 -> -100\n",
            "                 '阔': 56529 -> -100\n",
            "                 '腿': 56158 -> -100\n",
            "                 '裤': 56532 -> -100\n",
            "     '<|assistant|>': 64796 -> -100\n",
            "                  '': 30910 -> 30910\n",
            "                '\\n': 13 -> 13\n",
            "                  '': 30910 -> 30910\n",
            "                '宽松': 40833 -> 40833\n",
            "                 '的': 54530 -> 54530\n",
            "                 '阔': 56529 -> 56529\n",
            "                 '腿': 56158 -> 56158\n",
            "                 '裤': 56532 -> 56532\n",
            "                 '这': 54551 -> 54551\n",
            "                '两年': 33808 -> 33808\n",
            "                '真的': 32041 -> 32041\n",
            "                 '吸': 55360 -> 55360\n",
            "                 '粉': 55486 -> 55486\n",
            "                '不少': 32138 -> 32138\n",
            "                 '，': 31123 -> 31123\n",
            "                '明星': 32943 -> 32943\n",
            "                '时尚': 33481 -> 33481\n",
            "                 '达': 54880 -> 54880\n",
            "                '人的': 31664 -> 31664\n",
            "                '心头': 46565 -> 46565\n",
            "                 '爱': 54799 -> 54799\n",
            "                 '。': 31155 -> 31155\n",
            "                '毕竟': 33051 -> 33051\n",
            "                 '好': 54591 -> 54591\n",
            "                 '穿': 55432 -> 55432\n",
            "                '时尚': 33481 -> 33481\n",
            "                 '，': 31123 -> 31123\n",
            "                 '谁': 55622 -> 55622\n",
            "                '都能': 32904 -> 32904\n",
            "                 '穿': 55432 -> 55432\n",
            "                 '出': 54557 -> 54557\n",
            "                 '腿': 56158 -> 56158\n",
            "                 '长': 54625 -> 54625\n",
            "                 '2': 30943 -> 30943\n",
            "                 '米': 55055 -> 55055\n",
            "               '的效果': 35590 -> 35590\n",
            "                '宽松': 40833 -> 40833\n",
            "                 '的': 54530 -> 54530\n",
            "                 '裤': 56532 -> 56532\n",
            "                 '腿': 56158 -> 56158\n",
            "                 '，': 31123 -> 31123\n",
            "               '当然是': 48466 -> 48466\n",
            "                 '遮': 57148 -> 57148\n",
            "                 '肉': 55343 -> 55343\n",
            "                 '小': 54603 -> 54603\n",
            "                '能手': 49355 -> 49355\n",
            "                 '啊': 55674 -> 55674\n",
            "                 '。': 31155 -> 31155\n",
            "                '上身': 51605 -> 51605\n",
            "                 '随': 55119 -> 55119\n",
            "                 '性': 54642 -> 54642\n",
            "                '自然': 31799 -> 31799\n",
            "                 '不': 54535 -> 54535\n",
            "                 '拘': 57036 -> 57036\n",
            "                 '束': 55625 -> 55625\n",
            "                 '，': 31123 -> 31123\n",
            "                '面料': 46839 -> 46839\n",
            "                 '亲': 55113 -> 55113\n",
            "                 '肤': 56089 -> 56089\n",
            "                '舒适': 33894 -> 33894\n",
            "                 '贴': 55778 -> 55778\n",
            "                '身体': 31902 -> 31902\n",
            "                 '验': 55017 -> 55017\n",
            "                 '感': 54706 -> 54706\n",
            "                 '棒': 56382 -> 56382\n",
            "                 '棒': 56382 -> 56382\n",
            "                 '哒': 59230 -> 59230\n",
            "                 '。': 31155 -> 31155\n",
            "                 '系': 54712 -> 54712\n",
            "                 '带': 54882 -> 54882\n",
            "                '部分': 31726 -> 31726\n",
            "                '增加': 31917 -> 31917\n",
            "                '设计': 31735 -> 31735\n",
            "                '看点': 45032 -> 45032\n",
            "                 '，': 31123 -> 31123\n",
            "                 '还': 54656 -> 54656\n",
            "                 '让': 54772 -> 54772\n",
            "                '单品': 46539 -> 46539\n",
            "               '的设计': 34481 -> 34481\n",
            "                 '感': 54706 -> 54706\n",
            "                '更强': 43084 -> 43084\n",
            "                 '。': 31155 -> 31155\n",
            "                '腿部': 46799 -> 46799\n",
            "                '线条': 37216 -> 37216\n",
            "                 '若': 55351 -> 55351\n",
            "                 '隐': 55733 -> 55733\n",
            "                 '若': 55351 -> 55351\n",
            "                 '现': 54600 -> 54600\n",
            "                 '的': 54530 -> 54530\n",
            "                 '，': 31123 -> 31123\n",
            "                '性感': 40589 -> 40589\n",
            "                 '撩': 58521 -> 58521\n",
            "                 '人': 54533 -> 54533\n",
            "                 '。': 31155 -> 31155\n",
            "                '颜色': 33692 -> 33692\n",
            "                 '敲': 57004 -> 57004\n",
            "                '温柔': 34678 -> 34678\n",
            "                 '的': 54530 -> 54530\n",
            "                 '，': 31123 -> 31123\n",
            "                 '与': 54619 -> 54619\n",
            "                '裤子': 44722 -> 44722\n",
            "                '本身': 32754 -> 32754\n",
            "                 '所': 54626 -> 54626\n",
            "                '呈现': 33169 -> 33169\n",
            "               '的风格': 48084 -> 48084\n",
            "                '有点': 33149 -> 33149\n",
            "                 '反': 54955 -> 54955\n",
            "                 '差': 55342 -> 55342\n",
            "                 '萌': 56842 -> 56842\n",
            "                 '。': 31155 -> 31155\n",
            "                  '': 2 -> 2\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running training *****\n",
            "  Num examples = 114,599\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 12,000\n",
            "  Number of trainable parameters = 1,949,696\n",
            "{'loss': 4.7879, 'grad_norm': 2.0349009037017822, 'learning_rate': 4.995833333333333e-05, 'epoch': 0.0}\n",
            "{'loss': 4.6395, 'grad_norm': 2.472841501235962, 'learning_rate': 4.991666666666667e-05, 'epoch': 0.0}\n",
            "{'loss': 4.3969, 'grad_norm': 2.649840831756592, 'learning_rate': 4.9875000000000006e-05, 'epoch': 0.0}\n",
            "{'loss': 4.1219, 'grad_norm': 2.394176959991455, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}\n",
            "{'loss': 3.9053, 'grad_norm': 2.4392783641815186, 'learning_rate': 4.979166666666667e-05, 'epoch': 0.0}\n",
            "{'loss': 3.9236, 'grad_norm': 2.4296061992645264, 'learning_rate': 4.975e-05, 'epoch': 0.0}\n",
            "{'loss': 3.802, 'grad_norm': 2.4614601135253906, 'learning_rate': 4.970833333333333e-05, 'epoch': 0.0}\n",
            "{'loss': 3.8156, 'grad_norm': 2.266741991043091, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.01}\n",
            "{'loss': 3.693, 'grad_norm': 2.521538019180298, 'learning_rate': 4.962500000000001e-05, 'epoch': 0.01}\n",
            "{'loss': 3.691, 'grad_norm': 2.628293752670288, 'learning_rate': 4.958333333333334e-05, 'epoch': 0.01}\n",
            "{'loss': 3.7262, 'grad_norm': 2.758610248565674, 'learning_rate': 4.954166666666667e-05, 'epoch': 0.01}\n",
            "{'loss': 3.6947, 'grad_norm': 2.7778663635253906, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.01}\n",
            "{'loss': 3.6666, 'grad_norm': 3.2190563678741455, 'learning_rate': 4.9458333333333334e-05, 'epoch': 0.01}\n",
            "{'loss': 3.7043, 'grad_norm': 3.2585487365722656, 'learning_rate': 4.9416666666666664e-05, 'epoch': 0.01}\n",
            "{'loss': 3.699, 'grad_norm': 3.192579746246338, 'learning_rate': 4.937500000000001e-05, 'epoch': 0.01}\n",
            "{'loss': 3.6678, 'grad_norm': 3.4302849769592285, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.01}\n",
            "{'loss': 3.5826, 'grad_norm': 3.326917886734009, 'learning_rate': 4.929166666666667e-05, 'epoch': 0.01}\n",
            "{'loss': 3.6645, 'grad_norm': 3.5803167819976807, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.01}\n",
            "{'loss': 3.5674, 'grad_norm': 3.5025811195373535, 'learning_rate': 4.9208333333333335e-05, 'epoch': 0.01}\n",
            "{'loss': 3.6223, 'grad_norm': 3.8891491889953613, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.01}\n",
            "{'loss': 3.718, 'grad_norm': 3.8730175495147705, 'learning_rate': 4.9125e-05, 'epoch': 0.01}\n",
            "{'loss': 3.6635, 'grad_norm': 3.976773262023926, 'learning_rate': 4.908333333333334e-05, 'epoch': 0.02}\n",
            "{'loss': 3.5387, 'grad_norm': 3.8556911945343018, 'learning_rate': 4.904166666666667e-05, 'epoch': 0.02}\n",
            "{'loss': 3.6029, 'grad_norm': 4.401645660400391, 'learning_rate': 4.9e-05, 'epoch': 0.02}\n",
            "{'loss': 3.6432, 'grad_norm': 4.077970504760742, 'learning_rate': 4.8958333333333335e-05, 'epoch': 0.02}\n",
            "{'loss': 3.5467, 'grad_norm': 4.038915634155273, 'learning_rate': 4.891666666666667e-05, 'epoch': 0.02}\n",
            "{'loss': 3.6459, 'grad_norm': 4.372215270996094, 'learning_rate': 4.8875e-05, 'epoch': 0.02}\n",
            "{'loss': 3.7066, 'grad_norm': 3.796971559524536, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.02}\n",
            "{'loss': 3.5518, 'grad_norm': 3.745344877243042, 'learning_rate': 4.879166666666667e-05, 'epoch': 0.02}\n",
            "{'loss': 3.585, 'grad_norm': 5.008187770843506, 'learning_rate': 4.875e-05, 'epoch': 0.02}\n",
            "{'loss': 3.5748, 'grad_norm': 4.555666923522949, 'learning_rate': 4.8708333333333336e-05, 'epoch': 0.02}\n",
            "{'loss': 3.5635, 'grad_norm': 4.396651268005371, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.02}\n",
            "{'loss': 3.6377, 'grad_norm': 4.125415802001953, 'learning_rate': 4.8625e-05, 'epoch': 0.02}\n",
            "{'loss': 3.5484, 'grad_norm': 4.347980499267578, 'learning_rate': 4.858333333333333e-05, 'epoch': 0.02}\n",
            "{'loss': 3.4758, 'grad_norm': 4.430408000946045, 'learning_rate': 4.854166666666667e-05, 'epoch': 0.02}\n",
            "{'loss': 3.6383, 'grad_norm': 4.867156505584717, 'learning_rate': 4.85e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5242, 'grad_norm': 4.48130464553833, 'learning_rate': 4.845833333333334e-05, 'epoch': 0.03}\n",
            "{'loss': 3.56, 'grad_norm': 4.291398048400879, 'learning_rate': 4.8416666666666673e-05, 'epoch': 0.03}\n",
            "{'loss': 3.6756, 'grad_norm': 4.889401912689209, 'learning_rate': 4.8375000000000004e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5521, 'grad_norm': 5.09841251373291, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.03}\n",
            "{'loss': 3.4404, 'grad_norm': 4.459311485290527, 'learning_rate': 4.829166666666667e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5494, 'grad_norm': 5.071986198425293, 'learning_rate': 4.825e-05, 'epoch': 0.03}\n",
            "{'loss': 3.6264, 'grad_norm': 4.96187686920166, 'learning_rate': 4.820833333333333e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5066, 'grad_norm': 4.532068729400635, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.03}\n",
            "{'loss': 3.4723, 'grad_norm': 5.133116722106934, 'learning_rate': 4.8125000000000004e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5645, 'grad_norm': 5.324507236480713, 'learning_rate': 4.8083333333333334e-05, 'epoch': 0.03}\n",
            "{'loss': 3.6389, 'grad_norm': 5.549007892608643, 'learning_rate': 4.804166666666667e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5096, 'grad_norm': 5.158013820648193, 'learning_rate': 4.8e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5459, 'grad_norm': 6.2512030601501465, 'learning_rate': 4.795833333333333e-05, 'epoch': 0.03}\n",
            "{'loss': 3.5426, 'grad_norm': 4.81899356842041, 'learning_rate': 4.791666666666667e-05, 'epoch': 0.03}\n",
            "  4% 500/12000 [02:39<1:02:32,  3.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:03<00:03,  1.94s/it]\u001b[A\n",
            " 75% 3/4 [00:07<00:02,  2.49s/it]\u001b[A\n",
            "100% 4/4 [00:27<00:00,  9.24s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.675 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "                                         \n",
            "\u001b[A{'eval_rouge-1': 29.929328, 'eval_rouge-2': 6.4362059999999985, 'eval_rouge-l': 24.864306000000003, 'eval_bleu-4': 0.0328211824292434, 'eval_runtime': 32.6533, 'eval_samples_per_second': 1.531, 'eval_steps_per_second': 0.122, 'epoch': 0.03}\n",
            "  4% 500/12000 [03:12<1:02:32,  3.06it/s]\n",
            "100% 4/4 [00:28<00:00,  9.24s/it]\u001b[A\n",
            "{'loss': 3.5734, 'grad_norm': 6.117218017578125, 'learning_rate': 4.7875000000000005e-05, 'epoch': 0.04}\n",
            "{'loss': 3.6346, 'grad_norm': 5.0699639320373535, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.04}\n",
            "{'loss': 3.4822, 'grad_norm': 5.660538196563721, 'learning_rate': 4.7791666666666665e-05, 'epoch': 0.04}\n",
            "{'loss': 3.5475, 'grad_norm': 5.136270999908447, 'learning_rate': 4.775e-05, 'epoch': 0.04}\n",
            "{'loss': 3.6107, 'grad_norm': 5.133636951446533, 'learning_rate': 4.770833333333334e-05, 'epoch': 0.04}\n",
            "{'loss': 3.5572, 'grad_norm': 5.770625591278076, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.04}\n",
            "{'loss': 3.5158, 'grad_norm': 5.670259952545166, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.04}\n",
            "{'loss': 3.4541, 'grad_norm': 5.065275192260742, 'learning_rate': 4.7583333333333336e-05, 'epoch': 0.04}\n",
            "{'loss': 3.5688, 'grad_norm': 4.975461483001709, 'learning_rate': 4.7541666666666666e-05, 'epoch': 0.04}\n",
            "{'loss': 3.498, 'grad_norm': 5.007696151733398, 'learning_rate': 4.75e-05, 'epoch': 0.04}\n",
            "{'loss': 3.407, 'grad_norm': 5.413633346557617, 'learning_rate': 4.745833333333334e-05, 'epoch': 0.04}\n",
            "{'loss': 3.49, 'grad_norm': 5.550515174865723, 'learning_rate': 4.741666666666667e-05, 'epoch': 0.04}\n",
            "{'loss': 3.4742, 'grad_norm': 5.799459457397461, 'learning_rate': 4.7375e-05, 'epoch': 0.04}\n",
            "{'loss': 3.5109, 'grad_norm': 6.283962726593018, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.04}\n",
            "{'loss': 3.5555, 'grad_norm': 5.559174537658691, 'learning_rate': 4.7291666666666666e-05, 'epoch': 0.05}\n",
            "{'loss': 3.584, 'grad_norm': 5.379989147186279, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4627, 'grad_norm': 6.037600517272949, 'learning_rate': 4.720833333333334e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4445, 'grad_norm': 5.9866838455200195, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.05}\n",
            "{'loss': 3.5787, 'grad_norm': 6.946170806884766, 'learning_rate': 4.7125e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4979, 'grad_norm': 4.898787975311279, 'learning_rate': 4.708333333333334e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4236, 'grad_norm': 5.6439104080200195, 'learning_rate': 4.704166666666667e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4885, 'grad_norm': 5.980292320251465, 'learning_rate': 4.7e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4338, 'grad_norm': 5.429241180419922, 'learning_rate': 4.695833333333334e-05, 'epoch': 0.05}\n",
            "{'loss': 3.4814, 'grad_norm': 5.185725688934326, 'learning_rate': 4.691666666666667e-05, 'epoch': 0.05}\n",
            "{'loss': 3.567, 'grad_norm': 5.453772068023682, 'learning_rate': 4.6875e-05, 'epoch': 0.05}\n",
            "{'loss': 3.476, 'grad_norm': 5.626736164093018, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.05}\n",
            "{'loss': 3.5467, 'grad_norm': 5.926799774169922, 'learning_rate': 4.679166666666667e-05, 'epoch': 0.05}\n",
            "{'loss': 3.5562, 'grad_norm': 6.424917221069336, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.05}\n",
            "{'loss': 3.5729, 'grad_norm': 5.748524188995361, 'learning_rate': 4.6708333333333335e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5664, 'grad_norm': 5.405673027038574, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.06}\n",
            "{'loss': 3.483, 'grad_norm': 5.816681385040283, 'learning_rate': 4.6625e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5107, 'grad_norm': 6.30783748626709, 'learning_rate': 4.658333333333333e-05, 'epoch': 0.06}\n",
            "{'loss': 3.452, 'grad_norm': 6.02912712097168, 'learning_rate': 4.654166666666667e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5385, 'grad_norm': 5.871575832366943, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5572, 'grad_norm': 5.761322498321533, 'learning_rate': 4.6458333333333335e-05, 'epoch': 0.06}\n",
            "{'loss': 3.6033, 'grad_norm': 5.6890177726745605, 'learning_rate': 4.641666666666667e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5234, 'grad_norm': 5.591090202331543, 'learning_rate': 4.6375e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5637, 'grad_norm': 5.7283034324646, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.06}\n",
            "{'loss': 3.482, 'grad_norm': 5.6093668937683105, 'learning_rate': 4.629166666666667e-05, 'epoch': 0.06}\n",
            "{'loss': 3.4881, 'grad_norm': 6.469200611114502, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.06}\n",
            "{'loss': 3.4701, 'grad_norm': 5.595495700836182, 'learning_rate': 4.6208333333333336e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5867, 'grad_norm': 6.271209239959717, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.06}\n",
            "{'loss': 3.5592, 'grad_norm': 5.670622825622559, 'learning_rate': 4.6125e-05, 'epoch': 0.06}\n",
            "{'loss': 3.4303, 'grad_norm': 5.708285331726074, 'learning_rate': 4.608333333333333e-05, 'epoch': 0.07}\n",
            "{'loss': 3.4975, 'grad_norm': 6.13400936126709, 'learning_rate': 4.604166666666666e-05, 'epoch': 0.07}\n",
            "{'loss': 3.5492, 'grad_norm': 5.771601676940918, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.07}\n",
            "{'loss': 3.5143, 'grad_norm': 6.143970489501953, 'learning_rate': 4.595833333333334e-05, 'epoch': 0.07}\n",
            "{'loss': 3.4334, 'grad_norm': 7.1648268699646, 'learning_rate': 4.591666666666667e-05, 'epoch': 0.07}\n",
            "{'loss': 3.4264, 'grad_norm': 6.069414138793945, 'learning_rate': 4.5875000000000004e-05, 'epoch': 0.07}\n",
            "{'loss': 3.5623, 'grad_norm': 6.273177146911621, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.07}\n",
            "  8% 1000/12000 [05:48<57:44,  3.17it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.35s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.61s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 30.972848000000003, 'eval_rouge-2': 6.608348000000001, 'eval_rouge-l': 21.45549, 'eval_bleu-4': 0.029049321753105265, 'eval_runtime': 83.3854, 'eval_samples_per_second': 0.6, 'eval_steps_per_second': 0.048, 'epoch': 0.07}\n",
            "  8% 1000/12000 [07:11<57:44,  3.17it/s]\n",
            "100% 4/4 [01:02<00:00, 16.85s/it]\u001b[A\n",
            "{'loss': 3.5623, 'grad_norm': 5.846760272979736, 'learning_rate': 4.579166666666667e-05, 'epoch': 0.07}\n",
            "{'loss': 3.634, 'grad_norm': 6.303547382354736, 'learning_rate': 4.575e-05, 'epoch': 0.07}\n",
            "{'loss': 3.4555, 'grad_norm': 7.453877925872803, 'learning_rate': 4.570833333333334e-05, 'epoch': 0.07}\n",
            "{'loss': 3.5502, 'grad_norm': 6.351507663726807, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.07}\n",
            "{'loss': 3.501, 'grad_norm': 6.029567241668701, 'learning_rate': 4.5625e-05, 'epoch': 0.07}\n",
            "{'loss': 3.5514, 'grad_norm': 6.105125904083252, 'learning_rate': 4.5583333333333335e-05, 'epoch': 0.07}\n",
            "{'loss': 3.4275, 'grad_norm': 6.089144229888916, 'learning_rate': 4.554166666666667e-05, 'epoch': 0.07}\n",
            "{'loss': 3.5775, 'grad_norm': 7.011885166168213, 'learning_rate': 4.55e-05, 'epoch': 0.08}\n",
            "{'loss': 3.5205, 'grad_norm': 6.988209247589111, 'learning_rate': 4.545833333333334e-05, 'epoch': 0.08}\n",
            "{'loss': 3.5082, 'grad_norm': 6.139439105987549, 'learning_rate': 4.541666666666667e-05, 'epoch': 0.08}\n",
            "{'loss': 3.5555, 'grad_norm': 6.200723648071289, 'learning_rate': 4.5375e-05, 'epoch': 0.08}\n",
            "{'loss': 3.5127, 'grad_norm': 6.2946271896362305, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4102, 'grad_norm': 6.021121025085449, 'learning_rate': 4.529166666666667e-05, 'epoch': 0.08}\n",
            "{'loss': 3.5678, 'grad_norm': 7.0190839767456055, 'learning_rate': 4.525e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4506, 'grad_norm': 5.942239761352539, 'learning_rate': 4.520833333333334e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4336, 'grad_norm': 7.4353814125061035, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4764, 'grad_norm': 5.951571941375732, 'learning_rate': 4.5125e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4297, 'grad_norm': 6.521078586578369, 'learning_rate': 4.5083333333333336e-05, 'epoch': 0.08}\n",
            "{'loss': 3.5455, 'grad_norm': 6.196846008300781, 'learning_rate': 4.504166666666667e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4596, 'grad_norm': 5.840879440307617, 'learning_rate': 4.5e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4803, 'grad_norm': 6.174403190612793, 'learning_rate': 4.495833333333333e-05, 'epoch': 0.08}\n",
            "{'loss': 3.4871, 'grad_norm': 6.524205684661865, 'learning_rate': 4.491666666666667e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4027, 'grad_norm': 6.092230796813965, 'learning_rate': 4.4875e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4783, 'grad_norm': 6.13515043258667, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.09}\n",
            "{'loss': 3.5516, 'grad_norm': 6.917608737945557, 'learning_rate': 4.4791666666666673e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4254, 'grad_norm': 6.574598789215088, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4207, 'grad_norm': 6.455411434173584, 'learning_rate': 4.4708333333333334e-05, 'epoch': 0.09}\n",
            "{'loss': 3.499, 'grad_norm': 6.456821441650391, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.09}\n",
            "{'loss': 3.5902, 'grad_norm': 5.906487464904785, 'learning_rate': 4.4625e-05, 'epoch': 0.09}\n",
            "{'loss': 3.5297, 'grad_norm': 6.635456562042236, 'learning_rate': 4.458333333333334e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4953, 'grad_norm': 6.327722072601318, 'learning_rate': 4.454166666666667e-05, 'epoch': 0.09}\n",
            "{'loss': 3.5869, 'grad_norm': 6.175745010375977, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4924, 'grad_norm': 6.333286762237549, 'learning_rate': 4.4458333333333334e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4432, 'grad_norm': 6.178166389465332, 'learning_rate': 4.4416666666666664e-05, 'epoch': 0.09}\n",
            "{'loss': 3.5723, 'grad_norm': 6.660616397857666, 'learning_rate': 4.4375e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4512, 'grad_norm': 6.198232173919678, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.09}\n",
            "{'loss': 3.4988, 'grad_norm': 6.68245267868042, 'learning_rate': 4.429166666666667e-05, 'epoch': 0.1}\n",
            "{'loss': 3.5502, 'grad_norm': 6.529806137084961, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.1}\n",
            "{'loss': 3.4172, 'grad_norm': 5.889469623565674, 'learning_rate': 4.4208333333333335e-05, 'epoch': 0.1}\n",
            "{'loss': 3.3787, 'grad_norm': 6.679985523223877, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.1}\n",
            "{'loss': 3.5012, 'grad_norm': 6.4078874588012695, 'learning_rate': 4.4125e-05, 'epoch': 0.1}\n",
            "{'loss': 3.5355, 'grad_norm': 6.514187812805176, 'learning_rate': 4.408333333333334e-05, 'epoch': 0.1}\n",
            "{'loss': 3.4816, 'grad_norm': 6.739169120788574, 'learning_rate': 4.404166666666667e-05, 'epoch': 0.1}\n",
            "{'loss': 3.5457, 'grad_norm': 7.1917724609375, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.1}\n",
            "{'loss': 3.4311, 'grad_norm': 6.721700668334961, 'learning_rate': 4.3958333333333336e-05, 'epoch': 0.1}\n",
            "{'loss': 3.383, 'grad_norm': 6.011589050292969, 'learning_rate': 4.3916666666666666e-05, 'epoch': 0.1}\n",
            "{'loss': 3.4201, 'grad_norm': 6.38783597946167, 'learning_rate': 4.3875e-05, 'epoch': 0.1}\n",
            "{'loss': 3.3955, 'grad_norm': 6.642022609710693, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.1}\n",
            "{'loss': 3.5547, 'grad_norm': 7.461106777191162, 'learning_rate': 4.379166666666667e-05, 'epoch': 0.1}\n",
            "{'loss': 3.5279, 'grad_norm': 5.923962593078613, 'learning_rate': 4.375e-05, 'epoch': 0.1}\n",
            " 12% 1500/12000 [09:47<51:23,  3.40it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.40s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.72s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 32.405044000000004, 'eval_rouge-2': 7.154267999999999, 'eval_rouge-l': 21.769671999999996, 'eval_bleu-4': 0.028978608604325443, 'eval_runtime': 83.923, 'eval_samples_per_second': 0.596, 'eval_steps_per_second': 0.048, 'epoch': 0.1}\n",
            " 12% 1500/12000 [11:11<51:23,  3.40it/s]\n",
            "100% 4/4 [01:02<00:00, 16.93s/it]\u001b[A\n",
            "{'loss': 3.5357, 'grad_norm': 6.723570823669434, 'learning_rate': 4.3708333333333336e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4061, 'grad_norm': 6.436088562011719, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.11}\n",
            "{'loss': 3.5818, 'grad_norm': 6.566493034362793, 'learning_rate': 4.3625e-05, 'epoch': 0.11}\n",
            "{'loss': 3.424, 'grad_norm': 7.785519599914551, 'learning_rate': 4.358333333333334e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4156, 'grad_norm': 6.07545804977417, 'learning_rate': 4.354166666666667e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4498, 'grad_norm': 7.5551676750183105, 'learning_rate': 4.35e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4562, 'grad_norm': 8.704103469848633, 'learning_rate': 4.345833333333334e-05, 'epoch': 0.11}\n",
            "{'loss': 3.5711, 'grad_norm': 6.655175685882568, 'learning_rate': 4.341666666666667e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4623, 'grad_norm': 6.845372676849365, 'learning_rate': 4.3375000000000004e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4969, 'grad_norm': 6.224810600280762, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.11}\n",
            "{'loss': 3.5018, 'grad_norm': 6.623722076416016, 'learning_rate': 4.329166666666667e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4566, 'grad_norm': 6.342898845672607, 'learning_rate': 4.325e-05, 'epoch': 0.11}\n",
            "{'loss': 3.5027, 'grad_norm': 7.093743324279785, 'learning_rate': 4.320833333333333e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4705, 'grad_norm': 6.2354044914245605, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.11}\n",
            "{'loss': 3.4688, 'grad_norm': 6.239201068878174, 'learning_rate': 4.3125000000000005e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4094, 'grad_norm': 6.803304195404053, 'learning_rate': 4.3083333333333335e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4934, 'grad_norm': 6.351121425628662, 'learning_rate': 4.304166666666667e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4715, 'grad_norm': 6.387933731079102, 'learning_rate': 4.3e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4699, 'grad_norm': 6.6085309982299805, 'learning_rate': 4.295833333333333e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4379, 'grad_norm': 6.168446063995361, 'learning_rate': 4.291666666666667e-05, 'epoch': 0.12}\n",
            "{'loss': 3.3699, 'grad_norm': 7.243485927581787, 'learning_rate': 4.2875000000000005e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4361, 'grad_norm': 6.61067008972168, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4531, 'grad_norm': 6.728393077850342, 'learning_rate': 4.2791666666666666e-05, 'epoch': 0.12}\n",
            "{'loss': 3.3785, 'grad_norm': 7.710081100463867, 'learning_rate': 4.275e-05, 'epoch': 0.12}\n",
            "{'loss': 3.5498, 'grad_norm': 7.207906246185303, 'learning_rate': 4.270833333333333e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4707, 'grad_norm': 7.133011817932129, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4193, 'grad_norm': 7.139923095703125, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4072, 'grad_norm': 6.833367824554443, 'learning_rate': 4.2583333333333336e-05, 'epoch': 0.12}\n",
            "{'loss': 3.4898, 'grad_norm': 7.999753952026367, 'learning_rate': 4.2541666666666666e-05, 'epoch': 0.12}\n",
            "{'loss': 3.3305, 'grad_norm': 6.564010143280029, 'learning_rate': 4.25e-05, 'epoch': 0.13}\n",
            "{'loss': 3.5332, 'grad_norm': 7.037711143493652, 'learning_rate': 4.245833333333333e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4127, 'grad_norm': 6.947683334350586, 'learning_rate': 4.241666666666667e-05, 'epoch': 0.13}\n",
            "{'loss': 3.3703, 'grad_norm': 7.157931804656982, 'learning_rate': 4.237500000000001e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4557, 'grad_norm': 6.599977016448975, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.13}\n",
            "{'loss': 3.5053, 'grad_norm': 7.200444221496582, 'learning_rate': 4.229166666666667e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4857, 'grad_norm': 7.5103960037231445, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4123, 'grad_norm': 7.184917449951172, 'learning_rate': 4.2208333333333334e-05, 'epoch': 0.13}\n",
            "{'loss': 3.318, 'grad_norm': 7.351991176605225, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.13}\n",
            "{'loss': 3.3809, 'grad_norm': 6.644815921783447, 'learning_rate': 4.2125e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4885, 'grad_norm': 6.935301780700684, 'learning_rate': 4.208333333333334e-05, 'epoch': 0.13}\n",
            "{'loss': 3.3947, 'grad_norm': 6.928574562072754, 'learning_rate': 4.204166666666667e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4699, 'grad_norm': 6.763123035430908, 'learning_rate': 4.2e-05, 'epoch': 0.13}\n",
            "{'loss': 3.4213, 'grad_norm': 6.896332740783691, 'learning_rate': 4.1958333333333335e-05, 'epoch': 0.13}\n",
            "{'loss': 3.5918, 'grad_norm': 7.535679817199707, 'learning_rate': 4.191666666666667e-05, 'epoch': 0.14}\n",
            "{'loss': 3.5408, 'grad_norm': 6.3791890144348145, 'learning_rate': 4.1875e-05, 'epoch': 0.14}\n",
            "{'loss': 3.4152, 'grad_norm': 7.707828998565674, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.14}\n",
            "{'loss': 3.5676, 'grad_norm': 7.091618061065674, 'learning_rate': 4.179166666666667e-05, 'epoch': 0.14}\n",
            "{'loss': 3.4324, 'grad_norm': 6.863892078399658, 'learning_rate': 4.175e-05, 'epoch': 0.14}\n",
            "{'loss': 3.524, 'grad_norm': 7.091185569763184, 'learning_rate': 4.1708333333333335e-05, 'epoch': 0.14}\n",
            "{'loss': 3.5418, 'grad_norm': 7.45494270324707, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.14}\n",
            " 17% 2000/12000 [13:46<50:32,  3.30it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.45s/it]\u001b[A\n",
            " 75% 3/4 [00:24<00:07,  7.58s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.93249000000001, 'eval_rouge-2': 6.672838, 'eval_rouge-l': 24.993923999999996, 'eval_bleu-4': 0.030337137771960617, 'eval_runtime': 66.8424, 'eval_samples_per_second': 0.748, 'eval_steps_per_second': 0.06, 'epoch': 0.14}\n",
            " 17% 2000/12000 [14:53<50:32,  3.30it/s]\n",
            "100% 4/4 [00:45<00:00, 12.49s/it]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-2000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
            "Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "{'loss': 3.4215, 'grad_norm': 6.922163009643555, 'learning_rate': 4.1625e-05, 'epoch': 0.14}\n",
            "{'loss': 3.5268, 'grad_norm': 7.445827960968018, 'learning_rate': 4.158333333333333e-05, 'epoch': 0.14}\n",
            "{'loss': 3.408, 'grad_norm': 7.33494758605957, 'learning_rate': 4.154166666666667e-05, 'epoch': 0.14}\n",
            "{'loss': 3.4057, 'grad_norm': 7.177807807922363, 'learning_rate': 4.15e-05, 'epoch': 0.14}\n",
            "{'loss': 3.502, 'grad_norm': 7.286719799041748, 'learning_rate': 4.1458333333333336e-05, 'epoch': 0.14}\n",
            "{'loss': 3.4666, 'grad_norm': 7.140206336975098, 'learning_rate': 4.141666666666667e-05, 'epoch': 0.14}\n",
            "{'loss': 3.4637, 'grad_norm': 6.16768741607666, 'learning_rate': 4.1375e-05, 'epoch': 0.14}\n",
            "{'loss': 3.4584, 'grad_norm': 7.259016990661621, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4049, 'grad_norm': 6.626718997955322, 'learning_rate': 4.129166666666667e-05, 'epoch': 0.15}\n",
            "{'loss': 3.5398, 'grad_norm': 6.674529075622559, 'learning_rate': 4.125e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4557, 'grad_norm': 6.81615686416626, 'learning_rate': 4.120833333333334e-05, 'epoch': 0.15}\n",
            "{'loss': 3.5014, 'grad_norm': 6.390332221984863, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4922, 'grad_norm': 6.593604564666748, 'learning_rate': 4.1125000000000004e-05, 'epoch': 0.15}\n",
            "{'loss': 3.3184, 'grad_norm': 7.54276704788208, 'learning_rate': 4.1083333333333334e-05, 'epoch': 0.15}\n",
            "{'loss': 3.46, 'grad_norm': 7.400464057922363, 'learning_rate': 4.104166666666667e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4014, 'grad_norm': 7.631430149078369, 'learning_rate': 4.1e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4129, 'grad_norm': 6.654449462890625, 'learning_rate': 4.095833333333334e-05, 'epoch': 0.15}\n",
            "{'loss': 3.402, 'grad_norm': 6.886982440948486, 'learning_rate': 4.091666666666667e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4822, 'grad_norm': 7.021884441375732, 'learning_rate': 4.0875000000000004e-05, 'epoch': 0.15}\n",
            "{'loss': 3.3703, 'grad_norm': 6.360223293304443, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.15}\n",
            "{'loss': 3.4342, 'grad_norm': 7.470468997955322, 'learning_rate': 4.0791666666666664e-05, 'epoch': 0.15}\n",
            "{'loss': 3.3887, 'grad_norm': 6.966396808624268, 'learning_rate': 4.075e-05, 'epoch': 0.15}\n",
            "{'loss': 3.425, 'grad_norm': 7.646646499633789, 'learning_rate': 4.070833333333334e-05, 'epoch': 0.16}\n",
            "{'loss': 3.5162, 'grad_norm': 6.428891658782959, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4588, 'grad_norm': 7.346161842346191, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4375, 'grad_norm': 7.982985973358154, 'learning_rate': 4.0583333333333335e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4695, 'grad_norm': 6.7451252937316895, 'learning_rate': 4.0541666666666665e-05, 'epoch': 0.16}\n",
            "{'loss': 3.3754, 'grad_norm': 8.090262413024902, 'learning_rate': 4.05e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4641, 'grad_norm': 7.105796813964844, 'learning_rate': 4.045833333333334e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4287, 'grad_norm': 6.532352447509766, 'learning_rate': 4.041666666666667e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4348, 'grad_norm': 7.922720432281494, 'learning_rate': 4.0375e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4672, 'grad_norm': 7.853315830230713, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4736, 'grad_norm': 7.384737014770508, 'learning_rate': 4.0291666666666666e-05, 'epoch': 0.16}\n",
            "{'loss': 3.5965, 'grad_norm': 7.019911766052246, 'learning_rate': 4.025e-05, 'epoch': 0.16}\n",
            "{'loss': 3.5398, 'grad_norm': 7.4972968101501465, 'learning_rate': 4.020833333333334e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4955, 'grad_norm': 7.308690071105957, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.16}\n",
            "{'loss': 3.4586, 'grad_norm': 7.143680095672607, 'learning_rate': 4.0125e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4893, 'grad_norm': 7.258032321929932, 'learning_rate': 4.0083333333333336e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4424, 'grad_norm': 6.997491359710693, 'learning_rate': 4.0041666666666666e-05, 'epoch': 0.17}\n",
            "{'loss': 3.5158, 'grad_norm': 7.461607456207275, 'learning_rate': 4e-05, 'epoch': 0.17}\n",
            "{'loss': 3.5297, 'grad_norm': 8.285909652709961, 'learning_rate': 3.995833333333333e-05, 'epoch': 0.17}\n",
            "{'loss': 3.3289, 'grad_norm': 7.25491189956665, 'learning_rate': 3.991666666666667e-05, 'epoch': 0.17}\n",
            "{'loss': 3.3861, 'grad_norm': 7.469782829284668, 'learning_rate': 3.9875e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4414, 'grad_norm': 6.995786666870117, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4756, 'grad_norm': 6.806927680969238, 'learning_rate': 3.979166666666667e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4385, 'grad_norm': 7.468814373016357, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.17}\n",
            "{'loss': 3.366, 'grad_norm': 7.087006092071533, 'learning_rate': 3.9708333333333334e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4801, 'grad_norm': 6.817727565765381, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.17}\n",
            "{'loss': 3.4246, 'grad_norm': 7.019530296325684, 'learning_rate': 3.9625e-05, 'epoch': 0.17}\n",
            "{'loss': 3.3076, 'grad_norm': 7.8477959632873535, 'learning_rate': 3.958333333333333e-05, 'epoch': 0.17}\n",
            " 21% 2500/12000 [17:31<47:48,  3.31it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.38s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.66s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.115676000000004, 'eval_rouge-2': 6.679404, 'eval_rouge-l': 20.51946, 'eval_bleu-4': 0.026980731402293654, 'eval_runtime': 83.7907, 'eval_samples_per_second': 0.597, 'eval_steps_per_second': 0.048, 'epoch': 0.17}\n",
            " 21% 2500/12000 [18:54<47:48,  3.31it/s]\n",
            "100% 4/4 [01:02<00:00, 16.89s/it]\u001b[A\n",
            "{'loss': 3.4805, 'grad_norm': 7.379410743713379, 'learning_rate': 3.9541666666666675e-05, 'epoch': 0.18}\n",
            "{'loss': 3.3984, 'grad_norm': 8.16256046295166, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4635, 'grad_norm': 7.694952487945557, 'learning_rate': 3.9458333333333335e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4959, 'grad_norm': 6.831655502319336, 'learning_rate': 3.941666666666667e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4838, 'grad_norm': 6.815500259399414, 'learning_rate': 3.9375e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4416, 'grad_norm': 6.814408779144287, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.18}\n",
            "{'loss': 3.3576, 'grad_norm': 7.4463419914245605, 'learning_rate': 3.929166666666667e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4148, 'grad_norm': 7.459181308746338, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4158, 'grad_norm': 7.434828758239746, 'learning_rate': 3.9208333333333335e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4937, 'grad_norm': 7.1030378341674805, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4891, 'grad_norm': 6.865076065063477, 'learning_rate': 3.9125e-05, 'epoch': 0.18}\n",
            "{'loss': 3.5119, 'grad_norm': 7.095278263092041, 'learning_rate': 3.908333333333333e-05, 'epoch': 0.18}\n",
            "{'loss': 3.3484, 'grad_norm': 8.004966735839844, 'learning_rate': 3.904166666666667e-05, 'epoch': 0.18}\n",
            "{'loss': 3.2457, 'grad_norm': 6.989818096160889, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.18}\n",
            "{'loss': 3.3473, 'grad_norm': 6.934726238250732, 'learning_rate': 3.8958333333333336e-05, 'epoch': 0.18}\n",
            "{'loss': 3.4057, 'grad_norm': 7.1066412925720215, 'learning_rate': 3.8916666666666666e-05, 'epoch': 0.19}\n",
            "{'loss': 3.3822, 'grad_norm': 6.956634998321533, 'learning_rate': 3.8875e-05, 'epoch': 0.19}\n",
            "{'loss': 3.3604, 'grad_norm': 6.664865016937256, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.19}\n",
            "{'loss': 3.4375, 'grad_norm': 7.549713134765625, 'learning_rate': 3.879166666666667e-05, 'epoch': 0.19}\n",
            "{'loss': 3.3594, 'grad_norm': 7.79154109954834, 'learning_rate': 3.875e-05, 'epoch': 0.19}\n",
            "{'loss': 3.4896, 'grad_norm': 7.384038925170898, 'learning_rate': 3.870833333333334e-05, 'epoch': 0.19}\n",
            "{'loss': 3.4006, 'grad_norm': 7.273121356964111, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.19}\n",
            "{'loss': 3.5014, 'grad_norm': 8.025259971618652, 'learning_rate': 3.8625e-05, 'epoch': 0.19}\n",
            "{'loss': 3.3434, 'grad_norm': 7.195474624633789, 'learning_rate': 3.8583333333333334e-05, 'epoch': 0.19}\n",
            "{'loss': 3.4119, 'grad_norm': 6.758971691131592, 'learning_rate': 3.854166666666667e-05, 'epoch': 0.19}\n",
            "{'loss': 3.4736, 'grad_norm': 7.466369152069092, 'learning_rate': 3.85e-05, 'epoch': 0.19}\n",
            "{'loss': 3.5256, 'grad_norm': 7.802004814147949, 'learning_rate': 3.845833333333334e-05, 'epoch': 0.19}\n",
            "{'loss': 3.3846, 'grad_norm': 7.348686695098877, 'learning_rate': 3.841666666666667e-05, 'epoch': 0.19}\n",
            "{'loss': 3.3973, 'grad_norm': 7.37470006942749, 'learning_rate': 3.8375e-05, 'epoch': 0.19}\n",
            "{'loss': 3.4301, 'grad_norm': 7.22725772857666, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4164, 'grad_norm': 8.980328559875488, 'learning_rate': 3.829166666666667e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4469, 'grad_norm': 7.591631889343262, 'learning_rate': 3.825e-05, 'epoch': 0.2}\n",
            "{'loss': 3.5021, 'grad_norm': 7.0818352699279785, 'learning_rate': 3.820833333333334e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4619, 'grad_norm': 7.546565055847168, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4293, 'grad_norm': 8.70116901397705, 'learning_rate': 3.8125e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4793, 'grad_norm': 7.653975963592529, 'learning_rate': 3.8083333333333335e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4863, 'grad_norm': 7.448174476623535, 'learning_rate': 3.804166666666667e-05, 'epoch': 0.2}\n",
            "{'loss': 3.5039, 'grad_norm': 7.447747707366943, 'learning_rate': 3.8e-05, 'epoch': 0.2}\n",
            "{'loss': 3.4117, 'grad_norm': 8.297775268554688, 'learning_rate': 3.795833333333333e-05, 'epoch': 0.2}\n",
            "{'loss': 3.5063, 'grad_norm': 7.3584723472595215, 'learning_rate': 3.791666666666667e-05, 'epoch': 0.2}\n",
            "{'loss': 3.3645, 'grad_norm': 7.712412357330322, 'learning_rate': 3.7875e-05, 'epoch': 0.2}\n",
            "{'loss': 3.3713, 'grad_norm': 9.346907615661621, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.2}\n",
            "{'loss': 3.3537, 'grad_norm': 6.768780708312988, 'learning_rate': 3.779166666666667e-05, 'epoch': 0.2}\n",
            "{'loss': 3.3652, 'grad_norm': 7.0108256340026855, 'learning_rate': 3.775e-05, 'epoch': 0.21}\n",
            "{'loss': 3.334, 'grad_norm': 6.759932518005371, 'learning_rate': 3.770833333333333e-05, 'epoch': 0.21}\n",
            "{'loss': 3.5826, 'grad_norm': 8.063283920288086, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.21}\n",
            "{'loss': 3.474, 'grad_norm': 7.184852123260498, 'learning_rate': 3.7625e-05, 'epoch': 0.21}\n",
            "{'loss': 3.533, 'grad_norm': 9.74537181854248, 'learning_rate': 3.7583333333333337e-05, 'epoch': 0.21}\n",
            "{'loss': 3.4713, 'grad_norm': 7.435473442077637, 'learning_rate': 3.754166666666667e-05, 'epoch': 0.21}\n",
            "{'loss': 3.491, 'grad_norm': 7.402774333953857, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.21}\n",
            " 25% 3000/12000 [21:32<49:58,  3.00it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.39s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.68s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 32.384654, 'eval_rouge-2': 7.0230999999999995, 'eval_rouge-l': 24.855534, 'eval_bleu-4': 0.03357401700672795, 'eval_runtime': 48.8059, 'eval_samples_per_second': 1.024, 'eval_steps_per_second': 0.082, 'epoch': 0.21}\n",
            " 25% 3000/12000 [22:20<49:58,  3.00it/s]\n",
            "100% 4/4 [00:44<00:00, 10.16s/it]\u001b[A\n",
            "{'loss': 3.391, 'grad_norm': 7.328157424926758, 'learning_rate': 3.7458333333333334e-05, 'epoch': 0.21}\n",
            "{'loss': 3.2969, 'grad_norm': 7.836411476135254, 'learning_rate': 3.7416666666666664e-05, 'epoch': 0.21}\n",
            "{'loss': 3.4363, 'grad_norm': 8.437262535095215, 'learning_rate': 3.737500000000001e-05, 'epoch': 0.21}\n",
            "{'loss': 3.4674, 'grad_norm': 8.132207870483398, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.21}\n",
            "{'loss': 3.466, 'grad_norm': 7.490151405334473, 'learning_rate': 3.729166666666667e-05, 'epoch': 0.21}\n",
            "{'loss': 3.3904, 'grad_norm': 9.228489875793457, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.21}\n",
            "{'loss': 3.3828, 'grad_norm': 6.990716457366943, 'learning_rate': 3.7208333333333334e-05, 'epoch': 0.21}\n",
            "{'loss': 3.5107, 'grad_norm': 7.239073753356934, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4508, 'grad_norm': 7.8219733238220215, 'learning_rate': 3.7125e-05, 'epoch': 0.22}\n",
            "{'loss': 3.358, 'grad_norm': 7.5924601554870605, 'learning_rate': 3.708333333333334e-05, 'epoch': 0.22}\n",
            "{'loss': 3.3992, 'grad_norm': 8.71544075012207, 'learning_rate': 3.704166666666667e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4686, 'grad_norm': 6.64280891418457, 'learning_rate': 3.7e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4418, 'grad_norm': 8.15750789642334, 'learning_rate': 3.6958333333333335e-05, 'epoch': 0.22}\n",
            "{'loss': 3.3766, 'grad_norm': 7.1829609870910645, 'learning_rate': 3.6916666666666665e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4336, 'grad_norm': 7.816277980804443, 'learning_rate': 3.6875e-05, 'epoch': 0.22}\n",
            "{'loss': 3.5115, 'grad_norm': 7.920160293579102, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4609, 'grad_norm': 8.435883522033691, 'learning_rate': 3.679166666666667e-05, 'epoch': 0.22}\n",
            "{'loss': 3.5252, 'grad_norm': 8.74708080291748, 'learning_rate': 3.675e-05, 'epoch': 0.22}\n",
            "{'loss': 3.5295, 'grad_norm': 7.6266069412231445, 'learning_rate': 3.6708333333333336e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4602, 'grad_norm': 7.345535755157471, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4412, 'grad_norm': 7.8775715827941895, 'learning_rate': 3.6625e-05, 'epoch': 0.22}\n",
            "{'loss': 3.4961, 'grad_norm': 7.1607513427734375, 'learning_rate': 3.658333333333334e-05, 'epoch': 0.22}\n",
            "{'loss': 3.3982, 'grad_norm': 8.101521492004395, 'learning_rate': 3.654166666666667e-05, 'epoch': 0.23}\n",
            "{'loss': 3.5432, 'grad_norm': 7.5173115730285645, 'learning_rate': 3.65e-05, 'epoch': 0.23}\n",
            "{'loss': 3.4361, 'grad_norm': 7.9528021812438965, 'learning_rate': 3.6458333333333336e-05, 'epoch': 0.23}\n",
            "{'loss': 3.425, 'grad_norm': 7.929563522338867, 'learning_rate': 3.641666666666667e-05, 'epoch': 0.23}\n",
            "{'loss': 3.3791, 'grad_norm': 7.12906551361084, 'learning_rate': 3.6375e-05, 'epoch': 0.23}\n",
            "{'loss': 3.3916, 'grad_norm': 8.511087417602539, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.23}\n",
            "{'loss': 3.4855, 'grad_norm': 7.3711042404174805, 'learning_rate': 3.629166666666667e-05, 'epoch': 0.23}\n",
            "{'loss': 3.5158, 'grad_norm': 7.6241865158081055, 'learning_rate': 3.625e-05, 'epoch': 0.23}\n",
            "{'loss': 3.3977, 'grad_norm': 7.214699745178223, 'learning_rate': 3.620833333333333e-05, 'epoch': 0.23}\n",
            "{'loss': 3.3813, 'grad_norm': 8.345234870910645, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.23}\n",
            "{'loss': 3.341, 'grad_norm': 7.840791702270508, 'learning_rate': 3.6125000000000004e-05, 'epoch': 0.23}\n",
            "{'loss': 3.4002, 'grad_norm': 7.341105937957764, 'learning_rate': 3.6083333333333334e-05, 'epoch': 0.23}\n",
            "{'loss': 3.4139, 'grad_norm': 7.367846488952637, 'learning_rate': 3.604166666666667e-05, 'epoch': 0.23}\n",
            "{'loss': 3.4979, 'grad_norm': 7.542428016662598, 'learning_rate': 3.6e-05, 'epoch': 0.23}\n",
            "{'loss': 3.4129, 'grad_norm': 6.819802761077881, 'learning_rate': 3.595833333333333e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4367, 'grad_norm': 7.7528815269470215, 'learning_rate': 3.591666666666667e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4229, 'grad_norm': 7.45565128326416, 'learning_rate': 3.5875000000000005e-05, 'epoch': 0.24}\n",
            "{'loss': 3.3707, 'grad_norm': 7.693758010864258, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4893, 'grad_norm': 8.55594539642334, 'learning_rate': 3.5791666666666665e-05, 'epoch': 0.24}\n",
            "{'loss': 3.423, 'grad_norm': 8.19719409942627, 'learning_rate': 3.575e-05, 'epoch': 0.24}\n",
            "{'loss': 3.5467, 'grad_norm': 7.466741561889648, 'learning_rate': 3.570833333333333e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4053, 'grad_norm': 8.68558406829834, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.24}\n",
            "{'loss': 3.3164, 'grad_norm': 7.759739398956299, 'learning_rate': 3.5625000000000005e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4213, 'grad_norm': 9.934232711791992, 'learning_rate': 3.5583333333333335e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4271, 'grad_norm': 7.974366664886475, 'learning_rate': 3.5541666666666665e-05, 'epoch': 0.24}\n",
            "{'loss': 3.3775, 'grad_norm': 8.250133514404297, 'learning_rate': 3.55e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4479, 'grad_norm': 7.676655292510986, 'learning_rate': 3.545833333333333e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4404, 'grad_norm': 7.1894755363464355, 'learning_rate': 3.541666666666667e-05, 'epoch': 0.24}\n",
            " 29% 3500/12000 [24:57<43:48,  3.23it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.43s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.75s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 30.701707999999993, 'eval_rouge-2': 6.576006, 'eval_rouge-l': 22.925428, 'eval_bleu-4': 0.03051347506581264, 'eval_runtime': 49.5719, 'eval_samples_per_second': 1.009, 'eval_steps_per_second': 0.081, 'epoch': 0.24}\n",
            " 29% 3500/12000 [25:47<43:48,  3.23it/s]\n",
            "100% 4/4 [00:45<00:00, 10.51s/it]\u001b[A\n",
            "{'loss': 3.6158, 'grad_norm': 7.7577223777771, 'learning_rate': 3.5375e-05, 'epoch': 0.25}\n",
            "{'loss': 3.2437, 'grad_norm': 7.380252361297607, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.25}\n",
            "{'loss': 3.3838, 'grad_norm': 8.168140411376953, 'learning_rate': 3.5291666666666666e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4799, 'grad_norm': 9.2070894241333, 'learning_rate': 3.525e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4301, 'grad_norm': 7.655523300170898, 'learning_rate': 3.520833333333334e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4252, 'grad_norm': 8.379884719848633, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4871, 'grad_norm': 8.329029083251953, 'learning_rate': 3.5125e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4094, 'grad_norm': 8.956621170043945, 'learning_rate': 3.508333333333334e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4592, 'grad_norm': 8.580665588378906, 'learning_rate': 3.504166666666667e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4301, 'grad_norm': 9.283702850341797, 'learning_rate': 3.5e-05, 'epoch': 0.25}\n",
            "{'loss': 3.3219, 'grad_norm': 9.011598587036133, 'learning_rate': 3.495833333333334e-05, 'epoch': 0.25}\n",
            "{'loss': 3.3154, 'grad_norm': 8.761474609375, 'learning_rate': 3.491666666666667e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4783, 'grad_norm': 7.720954895019531, 'learning_rate': 3.4875e-05, 'epoch': 0.25}\n",
            "{'loss': 3.427, 'grad_norm': 7.284586429595947, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4227, 'grad_norm': 7.979736328125, 'learning_rate': 3.479166666666667e-05, 'epoch': 0.25}\n",
            "{'loss': 3.4775, 'grad_norm': 7.521091461181641, 'learning_rate': 3.475e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4623, 'grad_norm': 8.00402545928955, 'learning_rate': 3.4708333333333334e-05, 'epoch': 0.26}\n",
            "{'loss': 3.3494, 'grad_norm': 8.455974578857422, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.26}\n",
            "{'loss': 3.3691, 'grad_norm': 8.497404098510742, 'learning_rate': 3.4625e-05, 'epoch': 0.26}\n",
            "{'loss': 3.3828, 'grad_norm': 8.89033031463623, 'learning_rate': 3.458333333333333e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4881, 'grad_norm': 7.927440166473389, 'learning_rate': 3.454166666666667e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4469, 'grad_norm': 8.02566909790039, 'learning_rate': 3.45e-05, 'epoch': 0.26}\n",
            "{'loss': 3.3496, 'grad_norm': 7.963437080383301, 'learning_rate': 3.4458333333333335e-05, 'epoch': 0.26}\n",
            "{'loss': 3.384, 'grad_norm': 7.903534412384033, 'learning_rate': 3.441666666666667e-05, 'epoch': 0.26}\n",
            "{'loss': 3.2563, 'grad_norm': 8.308839797973633, 'learning_rate': 3.4375e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4139, 'grad_norm': 8.025620460510254, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4594, 'grad_norm': 8.613378524780273, 'learning_rate': 3.429166666666667e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4939, 'grad_norm': 8.33172607421875, 'learning_rate': 3.4250000000000006e-05, 'epoch': 0.26}\n",
            "{'loss': 3.4494, 'grad_norm': 8.993351936340332, 'learning_rate': 3.4208333333333336e-05, 'epoch': 0.26}\n",
            "{'loss': 3.5053, 'grad_norm': 8.09587287902832, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.27}\n",
            "{'loss': 3.3762, 'grad_norm': 8.31381607055664, 'learning_rate': 3.4125e-05, 'epoch': 0.27}\n",
            "{'loss': 3.3824, 'grad_norm': 7.747307300567627, 'learning_rate': 3.408333333333333e-05, 'epoch': 0.27}\n",
            "{'loss': 3.3977, 'grad_norm': 8.643302917480469, 'learning_rate': 3.404166666666666e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4566, 'grad_norm': 7.440567493438721, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4152, 'grad_norm': 8.079309463500977, 'learning_rate': 3.3958333333333337e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4455, 'grad_norm': 7.790388584136963, 'learning_rate': 3.391666666666667e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4311, 'grad_norm': 7.843171119689941, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4887, 'grad_norm': 11.089496612548828, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.27}\n",
            "{'loss': 3.3922, 'grad_norm': 7.49348258972168, 'learning_rate': 3.3791666666666664e-05, 'epoch': 0.27}\n",
            "{'loss': 3.3375, 'grad_norm': 7.185400009155273, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4875, 'grad_norm': 7.64655876159668, 'learning_rate': 3.370833333333334e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4637, 'grad_norm': 8.00117015838623, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.27}\n",
            "{'loss': 3.4303, 'grad_norm': 7.615319728851318, 'learning_rate': 3.3625000000000004e-05, 'epoch': 0.27}\n",
            "{'loss': 3.3363, 'grad_norm': 7.688708782196045, 'learning_rate': 3.3583333333333334e-05, 'epoch': 0.28}\n",
            "{'loss': 3.3568, 'grad_norm': 8.904176712036133, 'learning_rate': 3.3541666666666664e-05, 'epoch': 0.28}\n",
            "{'loss': 3.5037, 'grad_norm': 7.675138473510742, 'learning_rate': 3.35e-05, 'epoch': 0.28}\n",
            "{'loss': 3.3807, 'grad_norm': 8.473237991333008, 'learning_rate': 3.345833333333334e-05, 'epoch': 0.28}\n",
            "{'loss': 3.392, 'grad_norm': 7.555255889892578, 'learning_rate': 3.341666666666667e-05, 'epoch': 0.28}\n",
            "{'loss': 3.4533, 'grad_norm': 8.030136108398438, 'learning_rate': 3.3375e-05, 'epoch': 0.28}\n",
            "{'loss': 3.357, 'grad_norm': 7.621837139129639, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.28}\n",
            " 33% 4000/12000 [28:24<38:50,  3.43it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.37s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.68s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.88155, 'eval_rouge-2': 7.750807999999999, 'eval_rouge-l': 22.589586, 'eval_bleu-4': 0.03469032039351672, 'eval_runtime': 66.1533, 'eval_samples_per_second': 0.756, 'eval_steps_per_second': 0.06, 'epoch': 0.28}\n",
            " 33% 4000/12000 [29:30<38:50,  3.43it/s]\n",
            "100% 4/4 [00:44<00:00, 10.35s/it]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-4000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
            "Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "{'loss': 3.3703, 'grad_norm': 8.963595390319824, 'learning_rate': 3.329166666666667e-05, 'epoch': 0.28}\n",
            "{'loss': 3.3416, 'grad_norm': 8.253705978393555, 'learning_rate': 3.325e-05, 'epoch': 0.28}\n",
            "{'loss': 3.3773, 'grad_norm': 7.866513729095459, 'learning_rate': 3.320833333333334e-05, 'epoch': 0.28}\n",
            "{'loss': 3.4133, 'grad_norm': 7.657172203063965, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.28}\n",
            "{'loss': 3.3783, 'grad_norm': 8.049595832824707, 'learning_rate': 3.3125e-05, 'epoch': 0.28}\n",
            "{'loss': 3.4129, 'grad_norm': 8.050385475158691, 'learning_rate': 3.3083333333333336e-05, 'epoch': 0.28}\n",
            "{'loss': 3.4965, 'grad_norm': 8.591947555541992, 'learning_rate': 3.304166666666667e-05, 'epoch': 0.28}\n",
            "{'loss': 3.3918, 'grad_norm': 8.437461853027344, 'learning_rate': 3.3e-05, 'epoch': 0.28}\n",
            "{'loss': 3.4141, 'grad_norm': 7.81274938583374, 'learning_rate': 3.295833333333333e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4887, 'grad_norm': 8.896329879760742, 'learning_rate': 3.291666666666667e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4137, 'grad_norm': 8.935407638549805, 'learning_rate': 3.2875e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4117, 'grad_norm': 8.283696174621582, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.29}\n",
            "{'loss': 3.473, 'grad_norm': 8.812148094177246, 'learning_rate': 3.279166666666667e-05, 'epoch': 0.29}\n",
            "{'loss': 3.3678, 'grad_norm': 7.443080425262451, 'learning_rate': 3.275e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4254, 'grad_norm': 7.95133113861084, 'learning_rate': 3.270833333333333e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4467, 'grad_norm': 7.950660705566406, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4371, 'grad_norm': 8.4480562210083, 'learning_rate': 3.2625e-05, 'epoch': 0.29}\n",
            "{'loss': 3.3508, 'grad_norm': 8.48851203918457, 'learning_rate': 3.258333333333333e-05, 'epoch': 0.29}\n",
            "{'loss': 3.398, 'grad_norm': 8.062012672424316, 'learning_rate': 3.254166666666667e-05, 'epoch': 0.29}\n",
            "{'loss': 3.3684, 'grad_norm': 8.18072509765625, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.29}\n",
            "{'loss': 3.3994, 'grad_norm': 7.656703948974609, 'learning_rate': 3.2458333333333334e-05, 'epoch': 0.29}\n",
            "{'loss': 3.4377, 'grad_norm': 8.523551940917969, 'learning_rate': 3.2416666666666664e-05, 'epoch': 0.29}\n",
            "{'loss': 3.2979, 'grad_norm': 8.053936958312988, 'learning_rate': 3.2375e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4115, 'grad_norm': 8.162546157836914, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4262, 'grad_norm': 7.624654293060303, 'learning_rate': 3.229166666666667e-05, 'epoch': 0.3}\n",
            "{'loss': 3.3602, 'grad_norm': 7.883640289306641, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4428, 'grad_norm': 9.643707275390625, 'learning_rate': 3.2208333333333335e-05, 'epoch': 0.3}\n",
            "{'loss': 3.3809, 'grad_norm': 8.61371898651123, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4762, 'grad_norm': 8.17875862121582, 'learning_rate': 3.2125e-05, 'epoch': 0.3}\n",
            "{'loss': 3.3865, 'grad_norm': 9.603568077087402, 'learning_rate': 3.208333333333334e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4426, 'grad_norm': 7.931859016418457, 'learning_rate': 3.204166666666667e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4625, 'grad_norm': 7.904603958129883, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4719, 'grad_norm': 8.63028335571289, 'learning_rate': 3.1958333333333335e-05, 'epoch': 0.3}\n",
            "{'loss': 3.4047, 'grad_norm': 9.314797401428223, 'learning_rate': 3.1916666666666665e-05, 'epoch': 0.3}\n",
            "{'loss': 3.3971, 'grad_norm': 7.978718280792236, 'learning_rate': 3.1875e-05, 'epoch': 0.3}\n",
            "{'loss': 3.491, 'grad_norm': 8.81082820892334, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.3}\n",
            "{'loss': 3.3174, 'grad_norm': 8.335915565490723, 'learning_rate': 3.179166666666667e-05, 'epoch': 0.31}\n",
            "{'loss': 3.3992, 'grad_norm': 7.489131927490234, 'learning_rate': 3.175e-05, 'epoch': 0.31}\n",
            "{'loss': 3.3668, 'grad_norm': 7.853159427642822, 'learning_rate': 3.1708333333333336e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4736, 'grad_norm': 8.284833908081055, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4076, 'grad_norm': 7.614068031311035, 'learning_rate': 3.1624999999999996e-05, 'epoch': 0.31}\n",
            "{'loss': 3.409, 'grad_norm': 8.706597328186035, 'learning_rate': 3.158333333333334e-05, 'epoch': 0.31}\n",
            "{'loss': 3.3893, 'grad_norm': 8.365843772888184, 'learning_rate': 3.154166666666667e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4678, 'grad_norm': 7.830456256866455, 'learning_rate': 3.15e-05, 'epoch': 0.31}\n",
            "{'loss': 3.3838, 'grad_norm': 8.176560401916504, 'learning_rate': 3.145833333333334e-05, 'epoch': 0.31}\n",
            "{'loss': 3.3275, 'grad_norm': 7.579957008361816, 'learning_rate': 3.141666666666667e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4979, 'grad_norm': 7.997639179229736, 'learning_rate': 3.1375e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4889, 'grad_norm': 8.587176322937012, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4393, 'grad_norm': 8.395061492919922, 'learning_rate': 3.129166666666667e-05, 'epoch': 0.31}\n",
            "{'loss': 3.4357, 'grad_norm': 8.883062362670898, 'learning_rate': 3.125e-05, 'epoch': 0.31}\n",
            " 38% 4500/12000 [32:07<42:35,  2.93it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.40s/it]\u001b[A\n",
            " 75% 3/4 [00:24<00:07,  7.46s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.519389999999994, 'eval_rouge-2': 6.84183, 'eval_rouge-l': 24.855664, 'eval_bleu-4': 0.030826459417877608, 'eval_runtime': 31.7093, 'eval_samples_per_second': 1.577, 'eval_steps_per_second': 0.126, 'epoch': 0.31}\n",
            " 38% 4500/12000 [32:39<42:35,  2.93it/s]\n",
            "100% 4/4 [00:27<00:00,  5.76s/it]\u001b[A\n",
            "{'loss': 3.424, 'grad_norm': 8.099123001098633, 'learning_rate': 3.120833333333333e-05, 'epoch': 0.31}\n",
            "{'loss': 3.3258, 'grad_norm': 7.807807445526123, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.32}\n",
            "{'loss': 3.3814, 'grad_norm': 7.66702938079834, 'learning_rate': 3.1125000000000004e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4709, 'grad_norm': 8.228649139404297, 'learning_rate': 3.1083333333333334e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4041, 'grad_norm': 8.809590339660645, 'learning_rate': 3.104166666666667e-05, 'epoch': 0.32}\n",
            "{'loss': 3.3539, 'grad_norm': 7.520714282989502, 'learning_rate': 3.1e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4092, 'grad_norm': 9.153742790222168, 'learning_rate': 3.095833333333333e-05, 'epoch': 0.32}\n",
            "{'loss': 3.2309, 'grad_norm': 7.790054798126221, 'learning_rate': 3.091666666666667e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4367, 'grad_norm': 8.334535598754883, 'learning_rate': 3.0875000000000005e-05, 'epoch': 0.32}\n",
            "{'loss': 3.3861, 'grad_norm': 9.249211311340332, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4604, 'grad_norm': 8.58551025390625, 'learning_rate': 3.079166666666667e-05, 'epoch': 0.32}\n",
            "{'loss': 3.3387, 'grad_norm': 8.014785766601562, 'learning_rate': 3.075e-05, 'epoch': 0.32}\n",
            "{'loss': 3.3787, 'grad_norm': 9.813711166381836, 'learning_rate': 3.070833333333333e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4105, 'grad_norm': 7.6551313400268555, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.32}\n",
            "{'loss': 3.4193, 'grad_norm': 8.21662712097168, 'learning_rate': 3.0625000000000006e-05, 'epoch': 0.32}\n",
            "{'loss': 3.44, 'grad_norm': 9.612855911254883, 'learning_rate': 3.0583333333333336e-05, 'epoch': 0.33}\n",
            "{'loss': 3.3857, 'grad_norm': 8.05890941619873, 'learning_rate': 3.0541666666666666e-05, 'epoch': 0.33}\n",
            "{'loss': 3.382, 'grad_norm': 8.91466999053955, 'learning_rate': 3.05e-05, 'epoch': 0.33}\n",
            "{'loss': 3.4021, 'grad_norm': 8.391462326049805, 'learning_rate': 3.0458333333333333e-05, 'epoch': 0.33}\n",
            "{'loss': 3.4045, 'grad_norm': 8.67029857635498, 'learning_rate': 3.0416666666666666e-05, 'epoch': 0.33}\n",
            "{'loss': 3.3041, 'grad_norm': 8.23780345916748, 'learning_rate': 3.0375000000000003e-05, 'epoch': 0.33}\n",
            "{'loss': 3.334, 'grad_norm': 8.869136810302734, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.33}\n",
            "{'loss': 3.4045, 'grad_norm': 8.89188003540039, 'learning_rate': 3.0291666666666667e-05, 'epoch': 0.33}\n",
            "{'loss': 3.3838, 'grad_norm': 7.791681289672852, 'learning_rate': 3.025e-05, 'epoch': 0.33}\n",
            "{'loss': 3.2934, 'grad_norm': 8.070651054382324, 'learning_rate': 3.0208333333333334e-05, 'epoch': 0.33}\n",
            "{'loss': 3.4283, 'grad_norm': 8.513352394104004, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.33}\n",
            "{'loss': 3.4072, 'grad_norm': 8.715778350830078, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.33}\n",
            "{'loss': 3.3963, 'grad_norm': 8.806331634521484, 'learning_rate': 3.0083333333333337e-05, 'epoch': 0.33}\n",
            "{'loss': 3.3482, 'grad_norm': 7.868527889251709, 'learning_rate': 3.0041666666666667e-05, 'epoch': 0.33}\n",
            "{'loss': 3.5008, 'grad_norm': 8.643174171447754, 'learning_rate': 3e-05, 'epoch': 0.34}\n",
            "{'loss': 3.4059, 'grad_norm': 8.47138786315918, 'learning_rate': 2.9958333333333334e-05, 'epoch': 0.34}\n",
            "{'loss': 3.4156, 'grad_norm': 8.271966934204102, 'learning_rate': 2.991666666666667e-05, 'epoch': 0.34}\n",
            "{'loss': 3.4658, 'grad_norm': 8.734648704528809, 'learning_rate': 2.9875000000000004e-05, 'epoch': 0.34}\n",
            "{'loss': 3.3945, 'grad_norm': 8.378912925720215, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.34}\n",
            "{'loss': 3.426, 'grad_norm': 8.40664291381836, 'learning_rate': 2.9791666666666668e-05, 'epoch': 0.34}\n",
            "{'loss': 3.4986, 'grad_norm': 8.605703353881836, 'learning_rate': 2.975e-05, 'epoch': 0.34}\n",
            "{'loss': 3.5178, 'grad_norm': 9.090015411376953, 'learning_rate': 2.970833333333333e-05, 'epoch': 0.34}\n",
            "{'loss': 3.4773, 'grad_norm': 8.484611511230469, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.34}\n",
            "{'loss': 3.3996, 'grad_norm': 8.478415489196777, 'learning_rate': 2.9625000000000002e-05, 'epoch': 0.34}\n",
            "{'loss': 3.3617, 'grad_norm': 8.50989055633545, 'learning_rate': 2.9583333333333335e-05, 'epoch': 0.34}\n",
            "{'loss': 3.3459, 'grad_norm': 7.805386066436768, 'learning_rate': 2.954166666666667e-05, 'epoch': 0.34}\n",
            "{'loss': 3.3355, 'grad_norm': 8.13035774230957, 'learning_rate': 2.95e-05, 'epoch': 0.34}\n",
            "{'loss': 3.1752, 'grad_norm': 9.267200469970703, 'learning_rate': 2.9458333333333332e-05, 'epoch': 0.34}\n",
            "{'loss': 3.4646, 'grad_norm': 8.30557918548584, 'learning_rate': 2.941666666666667e-05, 'epoch': 0.34}\n",
            "{'loss': 3.3955, 'grad_norm': 8.171234130859375, 'learning_rate': 2.9375000000000003e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4158, 'grad_norm': 9.602657318115234, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4146, 'grad_norm': 8.812946319580078, 'learning_rate': 2.9291666666666666e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4422, 'grad_norm': 9.015359878540039, 'learning_rate': 2.925e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4008, 'grad_norm': 8.456592559814453, 'learning_rate': 2.9208333333333333e-05, 'epoch': 0.35}\n",
            "{'loss': 3.3055, 'grad_norm': 8.350253105163574, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.35}\n",
            " 42% 5000/12000 [35:15<37:58,  3.07it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.42s/it]\u001b[A\n",
            " 75% 3/4 [00:23<00:07,  7.39s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.505618, 'eval_rouge-2': 7.288707999999999, 'eval_rouge-l': 24.053834000000002, 'eval_bleu-4': 0.03565276499446147, 'eval_runtime': 48.8213, 'eval_samples_per_second': 1.024, 'eval_steps_per_second': 0.082, 'epoch': 0.35}\n",
            " 42% 5000/12000 [36:03<37:58,  3.07it/s]\n",
            "100% 4/4 [00:27<00:00,  5.79s/it]\u001b[A\n",
            "{'loss': 3.4676, 'grad_norm': 9.00825023651123, 'learning_rate': 2.9125000000000003e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4143, 'grad_norm': 8.214045524597168, 'learning_rate': 2.9083333333333333e-05, 'epoch': 0.35}\n",
            "{'loss': 3.3879, 'grad_norm': 8.681299209594727, 'learning_rate': 2.9041666666666667e-05, 'epoch': 0.35}\n",
            "{'loss': 3.3588, 'grad_norm': 8.329766273498535, 'learning_rate': 2.9e-05, 'epoch': 0.35}\n",
            "{'loss': 3.3572, 'grad_norm': 8.493687629699707, 'learning_rate': 2.8958333333333337e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4312, 'grad_norm': 8.430354118347168, 'learning_rate': 2.891666666666667e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4268, 'grad_norm': 8.042080879211426, 'learning_rate': 2.8875e-05, 'epoch': 0.35}\n",
            "{'loss': 3.4117, 'grad_norm': 9.481775283813477, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.35}\n",
            "{'loss': 3.3416, 'grad_norm': 8.592063903808594, 'learning_rate': 2.8791666666666667e-05, 'epoch': 0.36}\n",
            "{'loss': 3.4215, 'grad_norm': 8.244892120361328, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.36}\n",
            "{'loss': 3.4053, 'grad_norm': 8.168915748596191, 'learning_rate': 2.8708333333333338e-05, 'epoch': 0.36}\n",
            "{'loss': 3.4311, 'grad_norm': 8.031600952148438, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.36}\n",
            "{'loss': 3.341, 'grad_norm': 7.688055992126465, 'learning_rate': 2.8625e-05, 'epoch': 0.36}\n",
            "{'loss': 3.4031, 'grad_norm': 8.12837028503418, 'learning_rate': 2.8583333333333335e-05, 'epoch': 0.36}\n",
            "{'loss': 3.5289, 'grad_norm': 8.72146224975586, 'learning_rate': 2.8541666666666668e-05, 'epoch': 0.36}\n",
            "{'loss': 3.3662, 'grad_norm': 8.2032470703125, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.36}\n",
            "{'loss': 3.5148, 'grad_norm': 8.585492134094238, 'learning_rate': 2.845833333333334e-05, 'epoch': 0.36}\n",
            "{'loss': 3.3785, 'grad_norm': 8.493102073669434, 'learning_rate': 2.841666666666667e-05, 'epoch': 0.36}\n",
            "{'loss': 3.4234, 'grad_norm': 7.987987041473389, 'learning_rate': 2.8375000000000002e-05, 'epoch': 0.36}\n",
            "{'loss': 3.3027, 'grad_norm': 9.403534889221191, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.36}\n",
            "{'loss': 3.3797, 'grad_norm': 9.096755981445312, 'learning_rate': 2.8291666666666665e-05, 'epoch': 0.36}\n",
            "{'loss': 3.3447, 'grad_norm': 8.496381759643555, 'learning_rate': 2.825e-05, 'epoch': 0.36}\n",
            "{'loss': 3.3537, 'grad_norm': 8.994199752807617, 'learning_rate': 2.8208333333333336e-05, 'epoch': 0.37}\n",
            "{'loss': 3.4439, 'grad_norm': 8.448370933532715, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.37}\n",
            "{'loss': 3.4643, 'grad_norm': 8.704950332641602, 'learning_rate': 2.8125000000000003e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3687, 'grad_norm': 10.154099464416504, 'learning_rate': 2.8083333333333333e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3391, 'grad_norm': 9.255223274230957, 'learning_rate': 2.8041666666666666e-05, 'epoch': 0.37}\n",
            "{'loss': 3.4012, 'grad_norm': 8.266026496887207, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3451, 'grad_norm': 8.624957084655762, 'learning_rate': 2.7958333333333336e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3973, 'grad_norm': 7.704524040222168, 'learning_rate': 2.791666666666667e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3949, 'grad_norm': 8.171707153320312, 'learning_rate': 2.7875e-05, 'epoch': 0.37}\n",
            "{'loss': 3.4008, 'grad_norm': 8.182191848754883, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3432, 'grad_norm': 9.191495895385742, 'learning_rate': 2.7791666666666667e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3592, 'grad_norm': 8.561057090759277, 'learning_rate': 2.7750000000000004e-05, 'epoch': 0.37}\n",
            "{'loss': 3.398, 'grad_norm': 8.869668006896973, 'learning_rate': 2.7708333333333337e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3285, 'grad_norm': 8.702094078063965, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.37}\n",
            "{'loss': 3.3188, 'grad_norm': 8.281163215637207, 'learning_rate': 2.7625e-05, 'epoch': 0.37}\n",
            "{'loss': 3.4469, 'grad_norm': 8.908528327941895, 'learning_rate': 2.7583333333333334e-05, 'epoch': 0.38}\n",
            "{'loss': 3.3711, 'grad_norm': 8.466020584106445, 'learning_rate': 2.7541666666666664e-05, 'epoch': 0.38}\n",
            "{'loss': 3.5055, 'grad_norm': 9.264370918273926, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.38}\n",
            "{'loss': 3.4465, 'grad_norm': 8.57483959197998, 'learning_rate': 2.7458333333333334e-05, 'epoch': 0.38}\n",
            "{'loss': 3.317, 'grad_norm': 8.39588737487793, 'learning_rate': 2.7416666666666668e-05, 'epoch': 0.38}\n",
            "{'loss': 3.4529, 'grad_norm': 10.110206604003906, 'learning_rate': 2.7375e-05, 'epoch': 0.38}\n",
            "{'loss': 3.374, 'grad_norm': 9.270421028137207, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.38}\n",
            "{'loss': 3.4104, 'grad_norm': 8.883431434631348, 'learning_rate': 2.7291666666666665e-05, 'epoch': 0.38}\n",
            "{'loss': 3.3188, 'grad_norm': 8.72851276397705, 'learning_rate': 2.725e-05, 'epoch': 0.38}\n",
            "{'loss': 3.409, 'grad_norm': 8.508405685424805, 'learning_rate': 2.7208333333333335e-05, 'epoch': 0.38}\n",
            "{'loss': 3.423, 'grad_norm': 8.837032318115234, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.38}\n",
            "{'loss': 3.3369, 'grad_norm': 9.073646545410156, 'learning_rate': 2.7125000000000002e-05, 'epoch': 0.38}\n",
            "{'loss': 3.2549, 'grad_norm': 8.376094818115234, 'learning_rate': 2.7083333333333332e-05, 'epoch': 0.38}\n",
            " 46% 5500/12000 [38:39<34:09,  3.17it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:04<00:04,  2.47s/it]\u001b[A\n",
            " 75% 3/4 [00:07<00:02,  2.57s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.295132, 'eval_rouge-2': 7.166021999999999, 'eval_rouge-l': 25.170237999999994, 'eval_bleu-4': 0.036786469109075355, 'eval_runtime': 14.6983, 'eval_samples_per_second': 3.402, 'eval_steps_per_second': 0.272, 'epoch': 0.38}\n",
            " 46% 5500/12000 [38:54<34:09,  3.17it/s]\n",
            "100% 4/4 [00:10<00:00,  2.64s/it]\u001b[A\n",
            "{'loss': 3.3857, 'grad_norm': 8.840819358825684, 'learning_rate': 2.7041666666666672e-05, 'epoch': 0.38}\n",
            "{'loss': 3.3426, 'grad_norm': 8.51174259185791, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.39}\n",
            "{'loss': 3.5, 'grad_norm': 8.008975982666016, 'learning_rate': 2.6958333333333336e-05, 'epoch': 0.39}\n",
            "{'loss': 3.3371, 'grad_norm': 8.102448463439941, 'learning_rate': 2.691666666666667e-05, 'epoch': 0.39}\n",
            "{'loss': 3.4162, 'grad_norm': 8.335858345031738, 'learning_rate': 2.6875e-05, 'epoch': 0.39}\n",
            "{'loss': 3.4188, 'grad_norm': 8.215156555175781, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.39}\n",
            "{'loss': 3.3523, 'grad_norm': 8.599294662475586, 'learning_rate': 2.679166666666667e-05, 'epoch': 0.39}\n",
            "{'loss': 3.3537, 'grad_norm': 9.239845275878906, 'learning_rate': 2.6750000000000003e-05, 'epoch': 0.39}\n",
            "{'loss': 3.3385, 'grad_norm': 8.27116584777832, 'learning_rate': 2.6708333333333337e-05, 'epoch': 0.39}\n",
            "{'loss': 3.3658, 'grad_norm': 9.001293182373047, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.39}\n",
            "{'loss': 3.4207, 'grad_norm': 8.21695327758789, 'learning_rate': 2.6625e-05, 'epoch': 0.39}\n",
            "{'loss': 3.4053, 'grad_norm': 7.96497917175293, 'learning_rate': 2.6583333333333333e-05, 'epoch': 0.39}\n",
            "{'loss': 3.3682, 'grad_norm': 9.415375709533691, 'learning_rate': 2.654166666666667e-05, 'epoch': 0.39}\n",
            "{'loss': 3.373, 'grad_norm': 9.518004417419434, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.39}\n",
            "{'loss': 3.391, 'grad_norm': 9.693657875061035, 'learning_rate': 2.6458333333333334e-05, 'epoch': 0.39}\n",
            "{'loss': 3.4664, 'grad_norm': 8.699442863464355, 'learning_rate': 2.6416666666666667e-05, 'epoch': 0.4}\n",
            "{'loss': 3.282, 'grad_norm': 8.59726333618164, 'learning_rate': 2.6375e-05, 'epoch': 0.4}\n",
            "{'loss': 3.3885, 'grad_norm': 8.258679389953613, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.4}\n",
            "{'loss': 3.3715, 'grad_norm': 9.726119995117188, 'learning_rate': 2.629166666666667e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4408, 'grad_norm': 8.826631546020508, 'learning_rate': 2.625e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4113, 'grad_norm': 8.953927993774414, 'learning_rate': 2.6208333333333335e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4008, 'grad_norm': 8.684041976928711, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.4}\n",
            "{'loss': 3.2826, 'grad_norm': 8.732976913452148, 'learning_rate': 2.6124999999999998e-05, 'epoch': 0.4}\n",
            "{'loss': 3.3928, 'grad_norm': 8.396354675292969, 'learning_rate': 2.608333333333333e-05, 'epoch': 0.4}\n",
            "{'loss': 3.3055, 'grad_norm': 8.409679412841797, 'learning_rate': 2.604166666666667e-05, 'epoch': 0.4}\n",
            "{'loss': 3.5061, 'grad_norm': 9.097887992858887, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4088, 'grad_norm': 8.692168235778809, 'learning_rate': 2.5958333333333335e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4912, 'grad_norm': 8.320511817932129, 'learning_rate': 2.5916666666666665e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4119, 'grad_norm': 8.74872875213623, 'learning_rate': 2.5875e-05, 'epoch': 0.4}\n",
            "{'loss': 3.4289, 'grad_norm': 8.06661605834961, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.4}\n",
            "{'loss': 3.375, 'grad_norm': 8.68991756439209, 'learning_rate': 2.579166666666667e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3699, 'grad_norm': 8.472399711608887, 'learning_rate': 2.5750000000000002e-05, 'epoch': 0.41}\n",
            "{'loss': 3.2986, 'grad_norm': 9.673640251159668, 'learning_rate': 2.5708333333333336e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3277, 'grad_norm': 8.319573402404785, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3687, 'grad_norm': 8.693346977233887, 'learning_rate': 2.5625e-05, 'epoch': 0.41}\n",
            "{'loss': 3.2412, 'grad_norm': 9.10696792602539, 'learning_rate': 2.5583333333333336e-05, 'epoch': 0.41}\n",
            "{'loss': 3.4252, 'grad_norm': 8.71958065032959, 'learning_rate': 2.554166666666667e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3979, 'grad_norm': 9.112608909606934, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.41}\n",
            "{'loss': 3.4707, 'grad_norm': 10.599292755126953, 'learning_rate': 2.5458333333333333e-05, 'epoch': 0.41}\n",
            "{'loss': 3.4375, 'grad_norm': 8.11668872833252, 'learning_rate': 2.5416666666666667e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3291, 'grad_norm': 8.89309024810791, 'learning_rate': 2.5375e-05, 'epoch': 0.41}\n",
            "{'loss': 3.398, 'grad_norm': 8.914719581604004, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3348, 'grad_norm': 8.29206657409668, 'learning_rate': 2.529166666666667e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3615, 'grad_norm': 8.164604187011719, 'learning_rate': 2.525e-05, 'epoch': 0.41}\n",
            "{'loss': 3.3461, 'grad_norm': 8.741766929626465, 'learning_rate': 2.5208333333333334e-05, 'epoch': 0.42}\n",
            "{'loss': 3.2814, 'grad_norm': 8.000730514526367, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3785, 'grad_norm': 8.773969650268555, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.42}\n",
            "{'loss': 3.4016, 'grad_norm': 7.911925792694092, 'learning_rate': 2.5083333333333338e-05, 'epoch': 0.42}\n",
            "{'loss': 3.2988, 'grad_norm': 9.25596809387207, 'learning_rate': 2.5041666666666668e-05, 'epoch': 0.42}\n",
            "{'loss': 3.4432, 'grad_norm': 8.904555320739746, 'learning_rate': 2.5e-05, 'epoch': 0.42}\n",
            " 50% 6000/12000 [41:29<31:20,  3.19it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.38s/it]\u001b[A\n",
            " 75% 3/4 [00:23<00:07,  7.16s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 32.755880000000005, 'eval_rouge-2': 8.017527999999999, 'eval_rouge-l': 25.547482, 'eval_bleu-4': 0.03766622469834849, 'eval_runtime': 30.0842, 'eval_samples_per_second': 1.662, 'eval_steps_per_second': 0.133, 'epoch': 0.42}\n",
            " 50% 6000/12000 [41:59<31:20,  3.19it/s]\n",
            "100% 4/4 [00:26<00:00,  5.40s/it]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-6000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
            "Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "{'loss': 3.3836, 'grad_norm': 10.335926055908203, 'learning_rate': 2.4958333333333335e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3725, 'grad_norm': 8.259418487548828, 'learning_rate': 2.4916666666666668e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3219, 'grad_norm': 9.335460662841797, 'learning_rate': 2.4875e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3568, 'grad_norm': 8.102307319641113, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3373, 'grad_norm': 9.10983657836914, 'learning_rate': 2.479166666666667e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3322, 'grad_norm': 9.541298866271973, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3039, 'grad_norm': 8.899909973144531, 'learning_rate': 2.4708333333333332e-05, 'epoch': 0.42}\n",
            "{'loss': 3.3824, 'grad_norm': 8.563589096069336, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.42}\n",
            "{'loss': 3.2777, 'grad_norm': 8.255114555358887, 'learning_rate': 2.4625000000000002e-05, 'epoch': 0.43}\n",
            "{'loss': 3.423, 'grad_norm': 8.39042854309082, 'learning_rate': 2.4583333333333332e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3225, 'grad_norm': 9.424240112304688, 'learning_rate': 2.454166666666667e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3822, 'grad_norm': 8.805342674255371, 'learning_rate': 2.45e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3152, 'grad_norm': 9.056754112243652, 'learning_rate': 2.4458333333333336e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3561, 'grad_norm': 8.752764701843262, 'learning_rate': 2.441666666666667e-05, 'epoch': 0.43}\n",
            "{'loss': 3.4625, 'grad_norm': 8.304767608642578, 'learning_rate': 2.4375e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3904, 'grad_norm': 8.870223999023438, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.43}\n",
            "{'loss': 3.402, 'grad_norm': 7.9395670890808105, 'learning_rate': 2.4291666666666666e-05, 'epoch': 0.43}\n",
            "{'loss': 3.4439, 'grad_norm': 8.92935848236084, 'learning_rate': 2.425e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3152, 'grad_norm': 9.448368072509766, 'learning_rate': 2.4208333333333337e-05, 'epoch': 0.43}\n",
            "{'loss': 3.309, 'grad_norm': 8.432794570922852, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3135, 'grad_norm': 8.744795799255371, 'learning_rate': 2.4125e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3689, 'grad_norm': 8.791253089904785, 'learning_rate': 2.4083333333333337e-05, 'epoch': 0.43}\n",
            "{'loss': 3.3959, 'grad_norm': 8.937907218933105, 'learning_rate': 2.4041666666666667e-05, 'epoch': 0.43}\n",
            "{'loss': 3.4402, 'grad_norm': 9.330160140991211, 'learning_rate': 2.4e-05, 'epoch': 0.44}\n",
            "{'loss': 3.4428, 'grad_norm': 10.983626365661621, 'learning_rate': 2.3958333333333334e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3104, 'grad_norm': 8.878766059875488, 'learning_rate': 2.3916666666666668e-05, 'epoch': 0.44}\n",
            "{'loss': 3.4318, 'grad_norm': 9.353182792663574, 'learning_rate': 2.3875e-05, 'epoch': 0.44}\n",
            "{'loss': 3.4344, 'grad_norm': 9.429065704345703, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3084, 'grad_norm': 8.954804420471191, 'learning_rate': 2.3791666666666668e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3592, 'grad_norm': 8.830891609191895, 'learning_rate': 2.375e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3412, 'grad_norm': 9.138505935668945, 'learning_rate': 2.3708333333333335e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3264, 'grad_norm': 8.512742042541504, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.44}\n",
            "{'loss': 3.4777, 'grad_norm': 8.532915115356445, 'learning_rate': 2.3624999999999998e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3227, 'grad_norm': 8.716336250305176, 'learning_rate': 2.3583333333333335e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3408, 'grad_norm': 9.122018814086914, 'learning_rate': 2.354166666666667e-05, 'epoch': 0.44}\n",
            "{'loss': 3.2877, 'grad_norm': 8.194517135620117, 'learning_rate': 2.35e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3895, 'grad_norm': 8.49680233001709, 'learning_rate': 2.3458333333333335e-05, 'epoch': 0.44}\n",
            "{'loss': 3.3605, 'grad_norm': 8.956999778747559, 'learning_rate': 2.341666666666667e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3891, 'grad_norm': 10.108139991760254, 'learning_rate': 2.3375000000000002e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3754, 'grad_norm': 8.961915969848633, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3705, 'grad_norm': 9.260970115661621, 'learning_rate': 2.3291666666666666e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3408, 'grad_norm': 9.283947944641113, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.45}\n",
            "{'loss': 3.4061, 'grad_norm': 9.292313575744629, 'learning_rate': 2.3208333333333336e-05, 'epoch': 0.45}\n",
            "{'loss': 3.2648, 'grad_norm': 8.951590538024902, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3631, 'grad_norm': 9.613081932067871, 'learning_rate': 2.3125000000000003e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3801, 'grad_norm': 9.507155418395996, 'learning_rate': 2.3083333333333333e-05, 'epoch': 0.45}\n",
            "{'loss': 3.2703, 'grad_norm': 9.52114486694336, 'learning_rate': 2.3041666666666667e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3283, 'grad_norm': 8.231474876403809, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.45}\n",
            "{'loss': 3.4338, 'grad_norm': 8.40543270111084, 'learning_rate': 2.2958333333333333e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3168, 'grad_norm': 10.16950798034668, 'learning_rate': 2.2916666666666667e-05, 'epoch': 0.45}\n",
            " 54% 6500/12000 [44:37<29:37,  3.10it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:21<00:21, 10.51s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.84s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.255924, 'eval_rouge-2': 7.41121, 'eval_rouge-l': 23.737508000000002, 'eval_bleu-4': 0.035626757189279386, 'eval_runtime': 66.2729, 'eval_samples_per_second': 0.754, 'eval_steps_per_second': 0.06, 'epoch': 0.45}\n",
            " 54% 6500/12000 [45:43<29:37,  3.10it/s]\n",
            "100% 4/4 [00:44<00:00, 10.29s/it]\u001b[A\n",
            "{'loss': 3.3645, 'grad_norm': 8.943614959716797, 'learning_rate': 2.2875e-05, 'epoch': 0.45}\n",
            "{'loss': 3.3779, 'grad_norm': 9.112807273864746, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3486, 'grad_norm': 8.717092514038086, 'learning_rate': 2.2791666666666667e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3084, 'grad_norm': 9.181707382202148, 'learning_rate': 2.275e-05, 'epoch': 0.46}\n",
            "{'loss': 3.36, 'grad_norm': 10.06248664855957, 'learning_rate': 2.2708333333333334e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3064, 'grad_norm': 8.574888229370117, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.46}\n",
            "{'loss': 3.4646, 'grad_norm': 9.089262962341309, 'learning_rate': 2.2625e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3475, 'grad_norm': 8.961030960083008, 'learning_rate': 2.2583333333333335e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3158, 'grad_norm': 9.527206420898438, 'learning_rate': 2.2541666666666668e-05, 'epoch': 0.46}\n",
            "{'loss': 3.4773, 'grad_norm': 8.649795532226562, 'learning_rate': 2.25e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3242, 'grad_norm': 8.447258949279785, 'learning_rate': 2.2458333333333335e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3463, 'grad_norm': 9.687187194824219, 'learning_rate': 2.2416666666666665e-05, 'epoch': 0.46}\n",
            "{'loss': 3.4127, 'grad_norm': 9.288508415222168, 'learning_rate': 2.2375000000000002e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3682, 'grad_norm': 10.409013748168945, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.46}\n",
            "{'loss': 3.2824, 'grad_norm': 8.689215660095215, 'learning_rate': 2.229166666666667e-05, 'epoch': 0.46}\n",
            "{'loss': 3.3934, 'grad_norm': 8.314345359802246, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.46}\n",
            "{'loss': 3.4371, 'grad_norm': 8.773014068603516, 'learning_rate': 2.2208333333333332e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3863, 'grad_norm': 9.732288360595703, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3127, 'grad_norm': 8.91024112701416, 'learning_rate': 2.2125000000000002e-05, 'epoch': 0.47}\n",
            "{'loss': 3.4115, 'grad_norm': 9.127005577087402, 'learning_rate': 2.2083333333333333e-05, 'epoch': 0.47}\n",
            "{'loss': 3.4043, 'grad_norm': 8.339394569396973, 'learning_rate': 2.204166666666667e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3891, 'grad_norm': 9.213425636291504, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3443, 'grad_norm': 9.760063171386719, 'learning_rate': 2.1958333333333333e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3807, 'grad_norm': 9.019414901733398, 'learning_rate': 2.191666666666667e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3783, 'grad_norm': 8.904106140136719, 'learning_rate': 2.1875e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3641, 'grad_norm': 9.309220314025879, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.47}\n",
            "{'loss': 3.2697, 'grad_norm': 12.07619857788086, 'learning_rate': 2.179166666666667e-05, 'epoch': 0.47}\n",
            "{'loss': 3.5617, 'grad_norm': 8.25194263458252, 'learning_rate': 2.175e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3592, 'grad_norm': 10.537217140197754, 'learning_rate': 2.1708333333333334e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3406, 'grad_norm': 8.63985824584961, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.47}\n",
            "{'loss': 3.3393, 'grad_norm': 8.555950164794922, 'learning_rate': 2.1625e-05, 'epoch': 0.48}\n",
            "{'loss': 3.2428, 'grad_norm': 8.801703453063965, 'learning_rate': 2.1583333333333334e-05, 'epoch': 0.48}\n",
            "{'loss': 3.4225, 'grad_norm': 9.798711776733398, 'learning_rate': 2.1541666666666667e-05, 'epoch': 0.48}\n",
            "{'loss': 3.442, 'grad_norm': 8.155701637268066, 'learning_rate': 2.15e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3834, 'grad_norm': 10.279866218566895, 'learning_rate': 2.1458333333333334e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3967, 'grad_norm': 9.13328742980957, 'learning_rate': 2.1416666666666668e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3266, 'grad_norm': 8.83676815032959, 'learning_rate': 2.1375e-05, 'epoch': 0.48}\n",
            "{'loss': 3.2795, 'grad_norm': 8.601449012756348, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.48}\n",
            "{'loss': 3.4279, 'grad_norm': 8.041486740112305, 'learning_rate': 2.1291666666666668e-05, 'epoch': 0.48}\n",
            "{'loss': 3.2889, 'grad_norm': 9.276629447937012, 'learning_rate': 2.125e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3984, 'grad_norm': 8.416885375976562, 'learning_rate': 2.1208333333333335e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3713, 'grad_norm': 8.986825942993164, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3324, 'grad_norm': 8.871374130249023, 'learning_rate': 2.1125000000000002e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3295, 'grad_norm': 8.632649421691895, 'learning_rate': 2.1083333333333335e-05, 'epoch': 0.48}\n",
            "{'loss': 3.3734, 'grad_norm': 8.926530838012695, 'learning_rate': 2.104166666666667e-05, 'epoch': 0.49}\n",
            "{'loss': 3.4105, 'grad_norm': 8.950239181518555, 'learning_rate': 2.1e-05, 'epoch': 0.49}\n",
            "{'loss': 3.3004, 'grad_norm': 9.906085968017578, 'learning_rate': 2.0958333333333336e-05, 'epoch': 0.49}\n",
            "{'loss': 3.4289, 'grad_norm': 8.335620880126953, 'learning_rate': 2.091666666666667e-05, 'epoch': 0.49}\n",
            "{'loss': 3.4391, 'grad_norm': 8.148809432983398, 'learning_rate': 2.0875e-05, 'epoch': 0.49}\n",
            "{'loss': 3.4279, 'grad_norm': 8.821529388427734, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.49}\n",
            " 58% 7000/12000 [48:20<24:52,  3.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:04<00:04,  2.17s/it]\u001b[A\n",
            " 75% 3/4 [00:25<00:09,  9.95s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.898014000000003, 'eval_rouge-2': 7.293828000000001, 'eval_rouge-l': 24.467834000000003, 'eval_bleu-4': 0.037175277278528526, 'eval_runtime': 49.4552, 'eval_samples_per_second': 1.011, 'eval_steps_per_second': 0.081, 'epoch': 0.49}\n",
            " 58% 7000/12000 [49:10<24:52,  3.35it/s]\n",
            "100% 4/4 [00:28<00:00,  7.25s/it]\u001b[A\n",
            "{'loss': 3.2545, 'grad_norm': 9.561514854431152, 'learning_rate': 2.0791666666666666e-05, 'epoch': 0.49}\n",
            "{'loss': 3.3057, 'grad_norm': 10.144500732421875, 'learning_rate': 2.075e-05, 'epoch': 0.49}\n",
            "{'loss': 3.4391, 'grad_norm': 9.176508903503418, 'learning_rate': 2.0708333333333336e-05, 'epoch': 0.49}\n",
            "{'loss': 3.2732, 'grad_norm': 9.083965301513672, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.49}\n",
            "{'loss': 3.2877, 'grad_norm': 10.108901977539062, 'learning_rate': 2.0625e-05, 'epoch': 0.49}\n",
            "{'loss': 3.3547, 'grad_norm': 9.029128074645996, 'learning_rate': 2.0583333333333333e-05, 'epoch': 0.49}\n",
            "{'loss': 3.4154, 'grad_norm': 10.466850280761719, 'learning_rate': 2.0541666666666667e-05, 'epoch': 0.49}\n",
            "{'loss': 3.3297, 'grad_norm': 9.875975608825684, 'learning_rate': 2.05e-05, 'epoch': 0.49}\n",
            "{'loss': 3.2732, 'grad_norm': 9.365671157836914, 'learning_rate': 2.0458333333333334e-05, 'epoch': 0.49}\n",
            "{'loss': 3.3979, 'grad_norm': 9.604963302612305, 'learning_rate': 2.0416666666666667e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3107, 'grad_norm': 9.234313011169434, 'learning_rate': 2.0375e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3211, 'grad_norm': 9.582094192504883, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.5}\n",
            "{'loss': 3.2637, 'grad_norm': 8.451689720153809, 'learning_rate': 2.0291666666666667e-05, 'epoch': 0.5}\n",
            "{'loss': 3.4787, 'grad_norm': 9.030888557434082, 'learning_rate': 2.025e-05, 'epoch': 0.5}\n",
            "{'loss': 3.39, 'grad_norm': 9.275300979614258, 'learning_rate': 2.0208333333333334e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3863, 'grad_norm': 9.53856372833252, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.5}\n",
            "{'loss': 3.2814, 'grad_norm': 9.320165634155273, 'learning_rate': 2.0125e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3383, 'grad_norm': 9.277393341064453, 'learning_rate': 2.0083333333333335e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3418, 'grad_norm': 9.098088264465332, 'learning_rate': 2.0041666666666668e-05, 'epoch': 0.5}\n",
            "{'loss': 3.333, 'grad_norm': 9.917985916137695, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
            "{'loss': 3.4504, 'grad_norm': 9.70314884185791, 'learning_rate': 1.9958333333333335e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3135, 'grad_norm': 10.286406517028809, 'learning_rate': 1.9916666666666665e-05, 'epoch': 0.5}\n",
            "{'loss': 3.3316, 'grad_norm': 8.601263999938965, 'learning_rate': 1.9875000000000002e-05, 'epoch': 0.5}\n",
            "{'loss': 3.4059, 'grad_norm': 10.45569896697998, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3557, 'grad_norm': 8.689644813537598, 'learning_rate': 1.9791666666666665e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3305, 'grad_norm': 9.3811674118042, 'learning_rate': 1.9750000000000002e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3191, 'grad_norm': 8.87891960144043, 'learning_rate': 1.9708333333333336e-05, 'epoch': 0.51}\n",
            "{'loss': 3.4158, 'grad_norm': 9.340173721313477, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3674, 'grad_norm': 8.902314186096191, 'learning_rate': 1.9625000000000003e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3275, 'grad_norm': 9.887078285217285, 'learning_rate': 1.9583333333333333e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3117, 'grad_norm': 8.85562515258789, 'learning_rate': 1.9541666666666666e-05, 'epoch': 0.51}\n",
            "{'loss': 3.2244, 'grad_norm': 9.553800582885742, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3039, 'grad_norm': 9.704777717590332, 'learning_rate': 1.9458333333333333e-05, 'epoch': 0.51}\n",
            "{'loss': 3.4557, 'grad_norm': 10.388752937316895, 'learning_rate': 1.9416666666666667e-05, 'epoch': 0.51}\n",
            "{'loss': 3.2574, 'grad_norm': 8.66451644897461, 'learning_rate': 1.9375e-05, 'epoch': 0.51}\n",
            "{'loss': 3.4232, 'grad_norm': 9.024033546447754, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.51}\n",
            "{'loss': 3.4756, 'grad_norm': 9.805901527404785, 'learning_rate': 1.9291666666666667e-05, 'epoch': 0.51}\n",
            "{'loss': 3.3566, 'grad_norm': 9.964077949523926, 'learning_rate': 1.925e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3447, 'grad_norm': 8.953927040100098, 'learning_rate': 1.9208333333333334e-05, 'epoch': 0.52}\n",
            "{'loss': 3.4813, 'grad_norm': 9.136296272277832, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3973, 'grad_norm': 8.747543334960938, 'learning_rate': 1.9125e-05, 'epoch': 0.52}\n",
            "{'loss': 3.4262, 'grad_norm': 8.251072883605957, 'learning_rate': 1.9083333333333334e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3355, 'grad_norm': 10.001490592956543, 'learning_rate': 1.9041666666666668e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3859, 'grad_norm': 9.752939224243164, 'learning_rate': 1.9e-05, 'epoch': 0.52}\n",
            "{'loss': 3.291, 'grad_norm': 9.654946327209473, 'learning_rate': 1.8958333333333334e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3678, 'grad_norm': 8.919618606567383, 'learning_rate': 1.8916666666666668e-05, 'epoch': 0.52}\n",
            "{'loss': 3.4221, 'grad_norm': 10.04883098602295, 'learning_rate': 1.8875e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3607, 'grad_norm': 9.767807006835938, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3627, 'grad_norm': 10.31782054901123, 'learning_rate': 1.8791666666666668e-05, 'epoch': 0.52}\n",
            "{'loss': 3.4664, 'grad_norm': 9.61850643157959, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.52}\n",
            " 62% 7500/12000 [51:45<22:35,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.40s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.67s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 31.60042, 'eval_rouge-2': 7.624964, 'eval_rouge-l': 24.164340000000003, 'eval_bleu-4': 0.03774296703553367, 'eval_runtime': 49.4738, 'eval_samples_per_second': 1.011, 'eval_steps_per_second': 0.081, 'epoch': 0.52}\n",
            " 62% 7500/12000 [52:34<22:35,  3.32it/s]\n",
            "100% 4/4 [00:44<00:00, 10.33s/it]\u001b[A\n",
            "{'loss': 3.4645, 'grad_norm': 10.021416664123535, 'learning_rate': 1.8708333333333332e-05, 'epoch': 0.52}\n",
            "{'loss': 3.2389, 'grad_norm': 8.93094539642334, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.52}\n",
            "{'loss': 3.3725, 'grad_norm': 11.967564582824707, 'learning_rate': 1.8625000000000002e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3578, 'grad_norm': 9.506233215332031, 'learning_rate': 1.8583333333333332e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3582, 'grad_norm': 9.739401817321777, 'learning_rate': 1.854166666666667e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3209, 'grad_norm': 8.882366180419922, 'learning_rate': 1.85e-05, 'epoch': 0.53}\n",
            "{'loss': 3.2031, 'grad_norm': 8.905333518981934, 'learning_rate': 1.8458333333333333e-05, 'epoch': 0.53}\n",
            "{'loss': 3.376, 'grad_norm': 9.366311073303223, 'learning_rate': 1.841666666666667e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3156, 'grad_norm': 9.879120826721191, 'learning_rate': 1.8375e-05, 'epoch': 0.53}\n",
            "{'loss': 3.35, 'grad_norm': 10.174365997314453, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3455, 'grad_norm': 8.689671516418457, 'learning_rate': 1.829166666666667e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3887, 'grad_norm': 10.878735542297363, 'learning_rate': 1.825e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3312, 'grad_norm': 9.891666412353516, 'learning_rate': 1.8208333333333337e-05, 'epoch': 0.53}\n",
            "{'loss': 3.359, 'grad_norm': 9.595132827758789, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.53}\n",
            "{'loss': 3.2805, 'grad_norm': 9.612438201904297, 'learning_rate': 1.8125e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3783, 'grad_norm': 9.066328048706055, 'learning_rate': 1.8083333333333337e-05, 'epoch': 0.53}\n",
            "{'loss': 3.3834, 'grad_norm': 10.183528900146484, 'learning_rate': 1.8041666666666667e-05, 'epoch': 0.54}\n",
            "{'loss': 3.3582, 'grad_norm': 9.30075740814209, 'learning_rate': 1.8e-05, 'epoch': 0.54}\n",
            "{'loss': 3.2574, 'grad_norm': 8.887006759643555, 'learning_rate': 1.7958333333333334e-05, 'epoch': 0.54}\n",
            "{'loss': 3.3629, 'grad_norm': 9.254888534545898, 'learning_rate': 1.7916666666666667e-05, 'epoch': 0.54}\n",
            "{'loss': 3.4934, 'grad_norm': 9.748248100280762, 'learning_rate': 1.7875e-05, 'epoch': 0.54}\n",
            "{'loss': 3.4225, 'grad_norm': 10.457795143127441, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.54}\n",
            "{'loss': 3.4959, 'grad_norm': 11.253549575805664, 'learning_rate': 1.7791666666666668e-05, 'epoch': 0.54}\n",
            "{'loss': 3.2893, 'grad_norm': 9.431668281555176, 'learning_rate': 1.775e-05, 'epoch': 0.54}\n",
            "{'loss': 3.36, 'grad_norm': 9.07503890991211, 'learning_rate': 1.7708333333333335e-05, 'epoch': 0.54}\n",
            "{'loss': 3.3408, 'grad_norm': 9.823447227478027, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.54}\n",
            "{'loss': 3.3285, 'grad_norm': 9.6212739944458, 'learning_rate': 1.7625e-05, 'epoch': 0.54}\n",
            "{'loss': 3.2766, 'grad_norm': 9.814043998718262, 'learning_rate': 1.7583333333333335e-05, 'epoch': 0.54}\n",
            "{'loss': 3.3662, 'grad_norm': 9.56549072265625, 'learning_rate': 1.754166666666667e-05, 'epoch': 0.54}\n",
            "{'loss': 3.2709, 'grad_norm': 9.58145523071289, 'learning_rate': 1.75e-05, 'epoch': 0.54}\n",
            "{'loss': 3.4752, 'grad_norm': 10.210297584533691, 'learning_rate': 1.7458333333333335e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3615, 'grad_norm': 9.964638710021973, 'learning_rate': 1.741666666666667e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3996, 'grad_norm': 9.639249801635742, 'learning_rate': 1.7375e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3303, 'grad_norm': 10.169443130493164, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.55}\n",
            "{'loss': 3.2934, 'grad_norm': 9.978142738342285, 'learning_rate': 1.7291666666666666e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3941, 'grad_norm': 8.8970308303833, 'learning_rate': 1.725e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3684, 'grad_norm': 9.303679466247559, 'learning_rate': 1.7208333333333336e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3838, 'grad_norm': 10.299060821533203, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.55}\n",
            "{'loss': 3.4043, 'grad_norm': 9.239801406860352, 'learning_rate': 1.7125000000000003e-05, 'epoch': 0.55}\n",
            "{'loss': 3.315, 'grad_norm': 9.395264625549316, 'learning_rate': 1.7083333333333333e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3301, 'grad_norm': 8.347877502441406, 'learning_rate': 1.7041666666666666e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3076, 'grad_norm': 9.283166885375977, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3615, 'grad_norm': 8.748573303222656, 'learning_rate': 1.6958333333333333e-05, 'epoch': 0.55}\n",
            "{'loss': 3.4131, 'grad_norm': 8.629496574401855, 'learning_rate': 1.6916666666666667e-05, 'epoch': 0.55}\n",
            "{'loss': 3.4959, 'grad_norm': 10.154048919677734, 'learning_rate': 1.6875000000000004e-05, 'epoch': 0.55}\n",
            "{'loss': 3.3148, 'grad_norm': 9.374258041381836, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.56}\n",
            "{'loss': 3.3961, 'grad_norm': 12.402621269226074, 'learning_rate': 1.6791666666666667e-05, 'epoch': 0.56}\n",
            "{'loss': 3.3625, 'grad_norm': 10.696329116821289, 'learning_rate': 1.675e-05, 'epoch': 0.56}\n",
            "{'loss': 3.1996, 'grad_norm': 9.085087776184082, 'learning_rate': 1.6708333333333334e-05, 'epoch': 0.56}\n",
            "{'loss': 3.4057, 'grad_norm': 8.715548515319824, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.56}\n",
            " 67% 8000/12000 [55:10<20:13,  3.29it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.42s/it]\u001b[A\n",
            " 75% 3/4 [00:24<00:07,  7.57s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 32.26151, 'eval_rouge-2': 7.284426, 'eval_rouge-l': 24.744338, 'eval_bleu-4': 0.035131753054939624, 'eval_runtime': 31.9675, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.125, 'epoch': 0.56}\n",
            " 67% 8000/12000 [55:42<20:13,  3.29it/s]\n",
            "100% 4/4 [00:27<00:00,  5.68s/it]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-8000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
            "Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "{'loss': 3.2256, 'grad_norm': 9.519042015075684, 'learning_rate': 1.6625e-05, 'epoch': 0.56}\n",
            "{'loss': 3.3018, 'grad_norm': 9.441678047180176, 'learning_rate': 1.6583333333333334e-05, 'epoch': 0.56}\n",
            "{'loss': 3.3799, 'grad_norm': 9.05912971496582, 'learning_rate': 1.6541666666666668e-05, 'epoch': 0.56}\n",
            "{'loss': 3.3775, 'grad_norm': 10.945135116577148, 'learning_rate': 1.65e-05, 'epoch': 0.56}\n",
            "{'loss': 3.3254, 'grad_norm': 9.96329116821289, 'learning_rate': 1.6458333333333335e-05, 'epoch': 0.56}\n",
            "{'loss': 3.317, 'grad_norm': 9.506138801574707, 'learning_rate': 1.6416666666666665e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2541, 'grad_norm': 9.541590690612793, 'learning_rate': 1.6375e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2213, 'grad_norm': 9.673948287963867, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2711, 'grad_norm': 9.56084156036377, 'learning_rate': 1.6291666666666665e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2725, 'grad_norm': 9.383260726928711, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.57}\n",
            "{'loss': 3.2648, 'grad_norm': 11.715953826904297, 'learning_rate': 1.6208333333333332e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3131, 'grad_norm': 9.304089546203613, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3045, 'grad_norm': 9.299877166748047, 'learning_rate': 1.6125000000000002e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3557, 'grad_norm': 9.451620101928711, 'learning_rate': 1.6083333333333332e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3635, 'grad_norm': 9.533038139343262, 'learning_rate': 1.604166666666667e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3496, 'grad_norm': 9.495318412780762, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3811, 'grad_norm': 9.217605590820312, 'learning_rate': 1.5958333333333333e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3994, 'grad_norm': 9.533327102661133, 'learning_rate': 1.591666666666667e-05, 'epoch': 0.57}\n",
            "{'loss': 3.2631, 'grad_norm': 9.44475269317627, 'learning_rate': 1.5875e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3871, 'grad_norm': 9.289715766906738, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3029, 'grad_norm': 9.081496238708496, 'learning_rate': 1.579166666666667e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3299, 'grad_norm': 9.804427146911621, 'learning_rate': 1.575e-05, 'epoch': 0.57}\n",
            "{'loss': 3.3686, 'grad_norm': 10.410423278808594, 'learning_rate': 1.5708333333333333e-05, 'epoch': 0.57}\n",
            "{'loss': 3.2631, 'grad_norm': 9.740545272827148, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4322, 'grad_norm': 9.326391220092773, 'learning_rate': 1.5625e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3467, 'grad_norm': 9.747554779052734, 'learning_rate': 1.5583333333333334e-05, 'epoch': 0.58}\n",
            "{'loss': 3.2943, 'grad_norm': 10.08162784576416, 'learning_rate': 1.5541666666666667e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3438, 'grad_norm': 9.764753341674805, 'learning_rate': 1.55e-05, 'epoch': 0.58}\n",
            "{'loss': 3.2643, 'grad_norm': 10.46626091003418, 'learning_rate': 1.5458333333333334e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3453, 'grad_norm': 9.621771812438965, 'learning_rate': 1.5416666666666668e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3912, 'grad_norm': 9.587146759033203, 'learning_rate': 1.5375e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3475, 'grad_norm': 9.452970504760742, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3256, 'grad_norm': 10.992690086364746, 'learning_rate': 1.5291666666666668e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4377, 'grad_norm': 9.652986526489258, 'learning_rate': 1.525e-05, 'epoch': 0.58}\n",
            "{'loss': 3.432, 'grad_norm': 9.872213363647461, 'learning_rate': 1.5208333333333333e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3926, 'grad_norm': 9.83569049835205, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3443, 'grad_norm': 10.340362548828125, 'learning_rate': 1.5125e-05, 'epoch': 0.58}\n",
            "{'loss': 3.3961, 'grad_norm': 8.839614868164062, 'learning_rate': 1.5083333333333335e-05, 'epoch': 0.58}\n",
            "{'loss': 3.24, 'grad_norm': 9.889846801757812, 'learning_rate': 1.5041666666666669e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4555, 'grad_norm': 10.539793968200684, 'learning_rate': 1.5e-05, 'epoch': 0.59}\n",
            "{'loss': 3.2855, 'grad_norm': 10.96939754486084, 'learning_rate': 1.4958333333333336e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4221, 'grad_norm': 10.250176429748535, 'learning_rate': 1.4916666666666667e-05, 'epoch': 0.59}\n",
            "{'loss': 3.2623, 'grad_norm': 9.573655128479004, 'learning_rate': 1.4875e-05, 'epoch': 0.59}\n",
            "{'loss': 3.3486, 'grad_norm': 10.211852073669434, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.59}\n",
            "{'loss': 3.407, 'grad_norm': 11.015389442443848, 'learning_rate': 1.4791666666666668e-05, 'epoch': 0.59}\n",
            "{'loss': 3.3047, 'grad_norm': 9.372647285461426, 'learning_rate': 1.475e-05, 'epoch': 0.59}\n",
            "{'loss': 3.3893, 'grad_norm': 9.720134735107422, 'learning_rate': 1.4708333333333335e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4398, 'grad_norm': 9.79189395904541, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4768, 'grad_norm': 11.602910995483398, 'learning_rate': 1.4625e-05, 'epoch': 0.59}\n",
            "{'loss': 3.3676, 'grad_norm': 9.464808464050293, 'learning_rate': 1.4583333333333335e-05, 'epoch': 0.59}\n",
            " 71% 8500/12000 [58:19<18:15,  3.20it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.37s/it]\u001b[A\n",
            " 75% 3/4 [00:30<00:09,  9.93s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_rouge-1': 33.362606, 'eval_rouge-2': 7.588558000000001, 'eval_rouge-l': 25.216725999999998, 'eval_bleu-4': 0.0347386645789592, 'eval_runtime': 54.1781, 'eval_samples_per_second': 0.923, 'eval_steps_per_second': 0.074, 'epoch': 0.59}\n",
            " 71% 8500/12000 [59:14<18:15,  3.20it/s]\n",
            "100% 4/4 [00:32<00:00,  7.19s/it]\u001b[A\n",
            "{'loss': 3.3406, 'grad_norm': 10.128854751586914, 'learning_rate': 1.4541666666666667e-05, 'epoch': 0.59}\n",
            "{'loss': 3.2689, 'grad_norm': 9.956669807434082, 'learning_rate': 1.45e-05, 'epoch': 0.59}\n",
            "{'loss': 3.4152, 'grad_norm': 10.03983211517334, 'learning_rate': 1.4458333333333335e-05, 'epoch': 0.6}\n",
            "{'loss': 3.4035, 'grad_norm': 10.065520286560059, 'learning_rate': 1.4416666666666667e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3881, 'grad_norm': 9.780823707580566, 'learning_rate': 1.4374999999999999e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3625, 'grad_norm': 9.305632591247559, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3514, 'grad_norm': 10.226884841918945, 'learning_rate': 1.4291666666666667e-05, 'epoch': 0.6}\n",
            "{'loss': 3.2586, 'grad_norm': 10.219874382019043, 'learning_rate': 1.4249999999999999e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3424, 'grad_norm': 9.838892936706543, 'learning_rate': 1.4208333333333334e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3334, 'grad_norm': 10.637777328491211, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.6}\n",
            "{'loss': 3.1912, 'grad_norm': 12.505578994750977, 'learning_rate': 1.4125e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3164, 'grad_norm': 9.219945907592773, 'learning_rate': 1.4083333333333335e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3469, 'grad_norm': 8.718194961547852, 'learning_rate': 1.4041666666666666e-05, 'epoch': 0.6}\n",
            "{'loss': 3.4008, 'grad_norm': 10.39560317993164, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.6}\n",
            "{'loss': 3.2916, 'grad_norm': 9.7764253616333, 'learning_rate': 1.3958333333333335e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3213, 'grad_norm': 8.884620666503906, 'learning_rate': 1.3916666666666667e-05, 'epoch': 0.6}\n",
            "{'loss': 3.3543, 'grad_norm': 10.557657241821289, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.61}\n",
            "{'loss': 3.4248, 'grad_norm': 9.278861045837402, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3854, 'grad_norm': 11.534456253051758, 'learning_rate': 1.3791666666666667e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3383, 'grad_norm': 9.834403038024902, 'learning_rate': 1.3750000000000002e-05, 'epoch': 0.61}\n",
            "{'loss': 3.266, 'grad_norm': 10.546052932739258, 'learning_rate': 1.3708333333333334e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3666, 'grad_norm': 8.976975440979004, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.61}\n",
            "{'loss': 3.4143, 'grad_norm': 11.058023452758789, 'learning_rate': 1.3625e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3975, 'grad_norm': 9.689215660095215, 'learning_rate': 1.3583333333333334e-05, 'epoch': 0.61}\n",
            "{'loss': 3.4998, 'grad_norm': 10.278447151184082, 'learning_rate': 1.3541666666666666e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3701, 'grad_norm': 10.358592987060547, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.61}\n",
            "{'loss': 3.308, 'grad_norm': 9.4219970703125, 'learning_rate': 1.3458333333333335e-05, 'epoch': 0.61}\n",
            "{'loss': 3.2584, 'grad_norm': 8.8629732131958, 'learning_rate': 1.3416666666666666e-05, 'epoch': 0.61}\n",
            "{'loss': 3.4221, 'grad_norm': 11.063912391662598, 'learning_rate': 1.3375000000000002e-05, 'epoch': 0.61}\n",
            "{'loss': 3.2611, 'grad_norm': 9.424179077148438, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3029, 'grad_norm': 9.522748947143555, 'learning_rate': 1.3291666666666667e-05, 'epoch': 0.62}\n",
            "{'loss': 3.3479, 'grad_norm': 9.39261245727539, 'learning_rate': 1.3250000000000002e-05, 'epoch': 0.62}\n",
            "{'loss': 3.2684, 'grad_norm': 10.158690452575684, 'learning_rate': 1.3208333333333334e-05, 'epoch': 0.62}\n",
            "{'loss': 3.4055, 'grad_norm': 9.580305099487305, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.62}\n",
            "{'loss': 3.2977, 'grad_norm': 9.349167823791504, 'learning_rate': 1.3125e-05, 'epoch': 0.62}\n",
            "{'loss': 3.391, 'grad_norm': 10.656341552734375, 'learning_rate': 1.3083333333333334e-05, 'epoch': 0.62}\n",
            "{'loss': 3.248, 'grad_norm': 9.593972206115723, 'learning_rate': 1.3041666666666666e-05, 'epoch': 0.62}\n",
            "{'loss': 3.401, 'grad_norm': 8.730209350585938, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.62}\n",
            "{'loss': 3.4236, 'grad_norm': 9.510175704956055, 'learning_rate': 1.2958333333333333e-05, 'epoch': 0.62}\n",
            "{'loss': 3.3684, 'grad_norm': 10.602496147155762, 'learning_rate': 1.2916666666666668e-05, 'epoch': 0.62}\n",
            "{'loss': 3.3678, 'grad_norm': 9.82307243347168, 'learning_rate': 1.2875000000000001e-05, 'epoch': 0.62}\n",
            "{'loss': 3.402, 'grad_norm': 9.812233924865723, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.62}\n",
            "{'loss': 3.3951, 'grad_norm': 9.083528518676758, 'learning_rate': 1.2791666666666668e-05, 'epoch': 0.62}\n",
            "{'loss': 3.4121, 'grad_norm': 10.525107383728027, 'learning_rate': 1.2750000000000002e-05, 'epoch': 0.62}\n",
            "{'loss': 3.293, 'grad_norm': 9.492012977600098, 'learning_rate': 1.2708333333333333e-05, 'epoch': 0.62}\n",
            "{'loss': 3.4322, 'grad_norm': 9.456929206848145, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.63}\n",
            "{'loss': 3.2697, 'grad_norm': 9.957002639770508, 'learning_rate': 1.2625e-05, 'epoch': 0.63}\n",
            "{'loss': 3.258, 'grad_norm': 10.423332214355469, 'learning_rate': 1.2583333333333334e-05, 'epoch': 0.63}\n",
            "{'loss': 3.3262, 'grad_norm': 9.375377655029297, 'learning_rate': 1.2541666666666669e-05, 'epoch': 0.63}\n",
            "{'loss': 3.3631, 'grad_norm': 10.283307075500488, 'learning_rate': 1.25e-05, 'epoch': 0.63}\n",
            " 75% 9000/12000 [1:01:49<15:25,  3.24it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.44s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.75s/it]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_rouge-1': 31.898827999999995, 'eval_rouge-2': 7.019528, 'eval_rouge-l': 23.638308000000002, 'eval_bleu-4': 0.03180793129943749, 'eval_runtime': 66.3804, 'eval_samples_per_second': 0.753, 'eval_steps_per_second': 0.06, 'epoch': 0.63}\n",
            " 75% 9000/12000 [1:02:56<15:25,  3.24it/s]\n",
            "100% 4/4 [00:45<00:00, 10.42s/it]\u001b[A\n",
            "{'loss': 3.3961, 'grad_norm': 9.010346412658691, 'learning_rate': 1.2458333333333334e-05, 'epoch': 0.63}\n",
            "{'loss': 3.4459, 'grad_norm': 11.200197219848633, 'learning_rate': 1.2416666666666667e-05, 'epoch': 0.63}\n",
            "{'loss': 3.4092, 'grad_norm': 9.910736083984375, 'learning_rate': 1.2375000000000001e-05, 'epoch': 0.63}\n",
            "{'loss': 3.5264, 'grad_norm': 9.839547157287598, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.63}\n",
            "{'loss': 3.341, 'grad_norm': 9.21155834197998, 'learning_rate': 1.2291666666666666e-05, 'epoch': 0.63}\n",
            "{'loss': 3.3488, 'grad_norm': 10.04532241821289, 'learning_rate': 1.225e-05, 'epoch': 0.63}\n",
            "{'loss': 3.3961, 'grad_norm': 10.21260929107666, 'learning_rate': 1.2208333333333335e-05, 'epoch': 0.63}\n",
            "{'loss': 3.4199, 'grad_norm': 10.514732360839844, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.63}\n",
            "{'loss': 3.3682, 'grad_norm': 9.485635757446289, 'learning_rate': 1.2125e-05, 'epoch': 0.63}\n",
            "{'loss': 3.3268, 'grad_norm': 9.767146110534668, 'learning_rate': 1.2083333333333333e-05, 'epoch': 0.64}\n",
            "{'loss': 3.4133, 'grad_norm': 9.791702270507812, 'learning_rate': 1.2041666666666669e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3361, 'grad_norm': 9.50379753112793, 'learning_rate': 1.2e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3871, 'grad_norm': 9.614309310913086, 'learning_rate': 1.1958333333333334e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3715, 'grad_norm': 9.436162948608398, 'learning_rate': 1.1916666666666667e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3648, 'grad_norm': 9.553717613220215, 'learning_rate': 1.1875e-05, 'epoch': 0.64}\n",
            "{'loss': 3.2521, 'grad_norm': 10.244891166687012, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.64}\n",
            "{'loss': 3.398, 'grad_norm': 10.240519523620605, 'learning_rate': 1.1791666666666668e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3551, 'grad_norm': 9.796688079833984, 'learning_rate': 1.175e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3518, 'grad_norm': 10.174662590026855, 'learning_rate': 1.1708333333333334e-05, 'epoch': 0.64}\n",
            "{'loss': 3.2922, 'grad_norm': 10.79488754272461, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3969, 'grad_norm': 9.898863792419434, 'learning_rate': 1.1625000000000001e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3645, 'grad_norm': 10.34233283996582, 'learning_rate': 1.1583333333333333e-05, 'epoch': 0.64}\n",
            "{'loss': 3.4031, 'grad_norm': 10.191568374633789, 'learning_rate': 1.1541666666666667e-05, 'epoch': 0.64}\n",
            "{'loss': 3.3109, 'grad_norm': 9.63290786743164, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3461, 'grad_norm': 9.654179573059082, 'learning_rate': 1.1458333333333333e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3182, 'grad_norm': 9.913636207580566, 'learning_rate': 1.1416666666666667e-05, 'epoch': 0.65}\n",
            "{'loss': 3.2703, 'grad_norm': 10.26230239868164, 'learning_rate': 1.1375e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3529, 'grad_norm': 12.411186218261719, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.65}\n",
            "{'loss': 3.358, 'grad_norm': 10.401226043701172, 'learning_rate': 1.1291666666666667e-05, 'epoch': 0.65}\n",
            "{'loss': 3.4111, 'grad_norm': 8.965428352355957, 'learning_rate': 1.125e-05, 'epoch': 0.65}\n",
            "{'loss': 3.2402, 'grad_norm': 11.451605796813965, 'learning_rate': 1.1208333333333332e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3777, 'grad_norm': 10.310462951660156, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3406, 'grad_norm': 9.691191673278809, 'learning_rate': 1.1125000000000001e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3777, 'grad_norm': 10.172382354736328, 'learning_rate': 1.1083333333333335e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3381, 'grad_norm': 9.180278778076172, 'learning_rate': 1.1041666666666666e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3477, 'grad_norm': 10.476058959960938, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3443, 'grad_norm': 9.3333101272583, 'learning_rate': 1.0958333333333335e-05, 'epoch': 0.65}\n",
            "{'loss': 3.3963, 'grad_norm': 12.061216354370117, 'learning_rate': 1.0916666666666667e-05, 'epoch': 0.65}\n",
            "{'loss': 3.2799, 'grad_norm': 9.029901504516602, 'learning_rate': 1.0875e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3574, 'grad_norm': 9.675002098083496, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3725, 'grad_norm': 10.17197036743164, 'learning_rate': 1.0791666666666667e-05, 'epoch': 0.66}\n",
            "{'loss': 3.4221, 'grad_norm': 9.344491958618164, 'learning_rate': 1.075e-05, 'epoch': 0.66}\n",
            "{'loss': 3.2979, 'grad_norm': 9.351887702941895, 'learning_rate': 1.0708333333333334e-05, 'epoch': 0.66}\n",
            "{'loss': 3.4127, 'grad_norm': 9.379960060119629, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.66}\n",
            "{'loss': 3.4352, 'grad_norm': 9.973036766052246, 'learning_rate': 1.0625e-05, 'epoch': 0.66}\n",
            "{'loss': 3.242, 'grad_norm': 10.029987335205078, 'learning_rate': 1.0583333333333334e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3287, 'grad_norm': 10.607935905456543, 'learning_rate': 1.0541666666666668e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3633, 'grad_norm': 11.5032377243042, 'learning_rate': 1.05e-05, 'epoch': 0.66}\n",
            "{'loss': 3.2619, 'grad_norm': 9.88983154296875, 'learning_rate': 1.0458333333333335e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3516, 'grad_norm': 11.738797187805176, 'learning_rate': 1.0416666666666668e-05, 'epoch': 0.66}\n",
            " 79% 9500/12000 [1:05:31<12:35,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.39s/it]\u001b[A\n",
            " 75% 3/4 [00:25<00:08,  8.07s/it]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_rouge-1': 32.841954, 'eval_rouge-2': 7.142892, 'eval_rouge-l': 23.794570000000004, 'eval_bleu-4': 0.032193271865348275, 'eval_runtime': 50.6468, 'eval_samples_per_second': 0.987, 'eval_steps_per_second': 0.079, 'epoch': 0.66}\n",
            " 79% 9500/12000 [1:06:22<12:35,  3.31it/s]\n",
            "100% 4/4 [00:29<00:00,  6.37s/it]\u001b[A\n",
            "{'loss': 3.3301, 'grad_norm': 10.075494766235352, 'learning_rate': 1.0375e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3707, 'grad_norm': 9.369193077087402, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.66}\n",
            "{'loss': 3.3678, 'grad_norm': 14.460923194885254, 'learning_rate': 1.0291666666666667e-05, 'epoch': 0.67}\n",
            "{'loss': 3.3896, 'grad_norm': 9.220848083496094, 'learning_rate': 1.025e-05, 'epoch': 0.67}\n",
            "{'loss': 3.2783, 'grad_norm': 9.025032043457031, 'learning_rate': 1.0208333333333334e-05, 'epoch': 0.67}\n",
            "{'loss': 3.2578, 'grad_norm': 10.999484062194824, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.67}\n",
            "{'loss': 3.335, 'grad_norm': 9.66211986541748, 'learning_rate': 1.0125e-05, 'epoch': 0.67}\n",
            "{'loss': 3.3383, 'grad_norm': 9.732736587524414, 'learning_rate': 1.0083333333333334e-05, 'epoch': 0.67}\n",
            "{'loss': 3.2971, 'grad_norm': 9.276144027709961, 'learning_rate': 1.0041666666666667e-05, 'epoch': 0.67}\n",
            "{'loss': 3.4105, 'grad_norm': 11.299249649047852, 'learning_rate': 1e-05, 'epoch': 0.67}\n",
            "{'loss': 3.4002, 'grad_norm': 11.06368637084961, 'learning_rate': 9.958333333333333e-06, 'epoch': 0.67}\n",
            "{'loss': 3.3162, 'grad_norm': 10.851463317871094, 'learning_rate': 9.916666666666668e-06, 'epoch': 0.67}\n",
            "{'loss': 3.3059, 'grad_norm': 9.695714950561523, 'learning_rate': 9.875000000000001e-06, 'epoch': 0.67}\n",
            "{'loss': 3.3062, 'grad_norm': 9.82787799835205, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.67}\n",
            "{'loss': 3.4332, 'grad_norm': 9.44140911102295, 'learning_rate': 9.791666666666666e-06, 'epoch': 0.67}\n",
            "{'loss': 3.2783, 'grad_norm': 10.02868938446045, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.67}\n",
            "{'loss': 3.3762, 'grad_norm': 9.766453742980957, 'learning_rate': 9.708333333333333e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3336, 'grad_norm': 9.999837875366211, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.68}\n",
            "{'loss': 3.2922, 'grad_norm': 9.894607543945312, 'learning_rate': 9.625e-06, 'epoch': 0.68}\n",
            "{'loss': 3.2969, 'grad_norm': 9.66402530670166, 'learning_rate': 9.583333333333334e-06, 'epoch': 0.68}\n",
            "{'loss': 3.2648, 'grad_norm': 10.212839126586914, 'learning_rate': 9.541666666666667e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3477, 'grad_norm': 10.98170280456543, 'learning_rate': 9.5e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3994, 'grad_norm': 10.510943412780762, 'learning_rate': 9.458333333333334e-06, 'epoch': 0.68}\n",
            "{'loss': 3.282, 'grad_norm': 10.315082550048828, 'learning_rate': 9.416666666666667e-06, 'epoch': 0.68}\n",
            "{'loss': 3.2602, 'grad_norm': 9.413495063781738, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3016, 'grad_norm': 10.979242324829102, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3793, 'grad_norm': 9.911473274230957, 'learning_rate': 9.291666666666666e-06, 'epoch': 0.68}\n",
            "{'loss': 3.2902, 'grad_norm': 9.245550155639648, 'learning_rate': 9.25e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3166, 'grad_norm': 10.166413307189941, 'learning_rate': 9.208333333333335e-06, 'epoch': 0.68}\n",
            "{'loss': 3.285, 'grad_norm': 9.11349105834961, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.68}\n",
            "{'loss': 3.3109, 'grad_norm': 9.69871997833252, 'learning_rate': 9.125e-06, 'epoch': 0.68}\n",
            "{'loss': 3.4184, 'grad_norm': 10.40605354309082, 'learning_rate': 9.083333333333333e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3588, 'grad_norm': 10.393220901489258, 'learning_rate': 9.041666666666668e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3889, 'grad_norm': 11.409011840820312, 'learning_rate': 9e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3859, 'grad_norm': 10.168344497680664, 'learning_rate': 8.958333333333334e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3275, 'grad_norm': 11.732400894165039, 'learning_rate': 8.916666666666667e-06, 'epoch': 0.69}\n",
            "{'loss': 3.2742, 'grad_norm': 9.811854362487793, 'learning_rate': 8.875e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3256, 'grad_norm': 11.15781021118164, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.69}\n",
            "{'loss': 3.4668, 'grad_norm': 9.293089866638184, 'learning_rate': 8.791666666666667e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3369, 'grad_norm': 10.144396781921387, 'learning_rate': 8.75e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3062, 'grad_norm': 9.652434349060059, 'learning_rate': 8.708333333333334e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3299, 'grad_norm': 10.32416820526123, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3291, 'grad_norm': 9.519477844238281, 'learning_rate': 8.625e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3893, 'grad_norm': 9.26445484161377, 'learning_rate': 8.583333333333333e-06, 'epoch': 0.69}\n",
            "{'loss': 3.373, 'grad_norm': 9.277861595153809, 'learning_rate': 8.541666666666666e-06, 'epoch': 0.69}\n",
            "{'loss': 3.3127, 'grad_norm': 10.480239868164062, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3146, 'grad_norm': 9.546613693237305, 'learning_rate': 8.458333333333333e-06, 'epoch': 0.7}\n",
            "{'loss': 3.4287, 'grad_norm': 11.697637557983398, 'learning_rate': 8.416666666666667e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3852, 'grad_norm': 11.274413108825684, 'learning_rate': 8.375e-06, 'epoch': 0.7}\n",
            "{'loss': 3.291, 'grad_norm': 11.314046859741211, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.7}\n",
            " 83% 10000/12000 [1:08:59<09:48,  3.40it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:04<00:04,  2.15s/it]\u001b[A\n",
            " 75% 3/4 [00:25<00:09,  9.94s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_rouge-1': 33.25020000000001, 'eval_rouge-2': 7.455692, 'eval_rouge-l': 25.204387999999998, 'eval_bleu-4': 0.03400587260384175, 'eval_runtime': 50.4665, 'eval_samples_per_second': 0.991, 'eval_steps_per_second': 0.079, 'epoch': 0.7}\n",
            " 83% 10000/12000 [1:09:49<09:48,  3.40it/s]\n",
            "100% 4/4 [00:29<00:00,  7.62s/it]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-10000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
            "Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "{'loss': 3.3758, 'grad_norm': 10.144294738769531, 'learning_rate': 8.291666666666667e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3252, 'grad_norm': 10.783308982849121, 'learning_rate': 8.25e-06, 'epoch': 0.7}\n",
            "{'loss': 3.323, 'grad_norm': 10.235978126525879, 'learning_rate': 8.208333333333332e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3965, 'grad_norm': 9.819875717163086, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3869, 'grad_norm': 10.513472557067871, 'learning_rate': 8.125000000000001e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3312, 'grad_norm': 10.964887619018555, 'learning_rate': 8.083333333333333e-06, 'epoch': 0.7}\n",
            "{'loss': 3.2451, 'grad_norm': 11.413898468017578, 'learning_rate': 8.041666666666666e-06, 'epoch': 0.7}\n",
            "{'loss': 3.3734, 'grad_norm': 11.700461387634277, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.7}\n",
            "{'loss': 3.2354, 'grad_norm': 9.77025032043457, 'learning_rate': 7.958333333333335e-06, 'epoch': 0.7}\n",
            "{'loss': 3.4463, 'grad_norm': 9.87312126159668, 'learning_rate': 7.916666666666667e-06, 'epoch': 0.71}\n",
            "{'loss': 3.401, 'grad_norm': 9.778963088989258, 'learning_rate': 7.875e-06, 'epoch': 0.71}\n",
            "{'loss': 3.4031, 'grad_norm': 9.293861389160156, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.71}\n",
            "{'loss': 3.3633, 'grad_norm': 10.521340370178223, 'learning_rate': 7.791666666666667e-06, 'epoch': 0.71}\n",
            "{'loss': 3.4293, 'grad_norm': 10.098145484924316, 'learning_rate': 7.75e-06, 'epoch': 0.71}\n",
            "{'loss': 3.4566, 'grad_norm': 9.218873023986816, 'learning_rate': 7.708333333333334e-06, 'epoch': 0.71}\n",
            "{'loss': 3.2895, 'grad_norm': 11.29271125793457, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.71}\n",
            "{'loss': 3.3758, 'grad_norm': 10.69012451171875, 'learning_rate': 7.625e-06, 'epoch': 0.71}\n",
            "{'loss': 3.2531, 'grad_norm': 10.16430377960205, 'learning_rate': 7.583333333333334e-06, 'epoch': 0.71}\n",
            "{'loss': 3.3543, 'grad_norm': 10.574894905090332, 'learning_rate': 7.541666666666668e-06, 'epoch': 0.71}\n",
            "{'loss': 3.3098, 'grad_norm': 9.509882926940918, 'learning_rate': 7.5e-06, 'epoch': 0.71}\n",
            "{'loss': 3.3795, 'grad_norm': 11.425125122070312, 'learning_rate': 7.458333333333334e-06, 'epoch': 0.71}\n",
            "{'loss': 3.3676, 'grad_norm': 10.466569900512695, 'learning_rate': 7.416666666666668e-06, 'epoch': 0.71}\n",
            "{'loss': 3.34, 'grad_norm': 10.011604309082031, 'learning_rate': 7.375e-06, 'epoch': 0.71}\n",
            "{'loss': 3.2936, 'grad_norm': 10.137484550476074, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.71}\n",
            "{'loss': 3.2781, 'grad_norm': 10.246244430541992, 'learning_rate': 7.2916666666666674e-06, 'epoch': 0.72}\n",
            "{'loss': 3.3937, 'grad_norm': 10.149229049682617, 'learning_rate': 7.25e-06, 'epoch': 0.72}\n",
            "{'loss': 3.4326, 'grad_norm': 9.743165016174316, 'learning_rate': 7.2083333333333335e-06, 'epoch': 0.72}\n",
            "{'loss': 3.2887, 'grad_norm': 9.339123725891113, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.72}\n",
            "{'loss': 3.2568, 'grad_norm': 9.826937675476074, 'learning_rate': 7.1249999999999995e-06, 'epoch': 0.72}\n",
            "{'loss': 3.2832, 'grad_norm': 10.48206615447998, 'learning_rate': 7.083333333333334e-06, 'epoch': 0.72}\n",
            "{'loss': 3.2727, 'grad_norm': 9.486652374267578, 'learning_rate': 7.041666666666667e-06, 'epoch': 0.72}\n",
            "{'loss': 3.367, 'grad_norm': 9.241556167602539, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.72}\n",
            "{'loss': 3.357, 'grad_norm': 10.550020217895508, 'learning_rate': 6.958333333333333e-06, 'epoch': 0.72}\n",
            "{'loss': 3.3211, 'grad_norm': 10.170848846435547, 'learning_rate': 6.916666666666667e-06, 'epoch': 0.72}\n",
            "{'loss': 3.2738, 'grad_norm': 9.794364929199219, 'learning_rate': 6.875000000000001e-06, 'epoch': 0.72}\n",
            "{'loss': 3.2787, 'grad_norm': 9.69206428527832, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.72}\n",
            "{'loss': 3.352, 'grad_norm': 10.096928596496582, 'learning_rate': 6.791666666666667e-06, 'epoch': 0.72}\n",
            "{'loss': 3.4037, 'grad_norm': 10.10884952545166, 'learning_rate': 6.750000000000001e-06, 'epoch': 0.72}\n",
            "{'loss': 3.3727, 'grad_norm': 10.475662231445312, 'learning_rate': 6.708333333333333e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3582, 'grad_norm': 8.545998573303223, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3309, 'grad_norm': 10.484403610229492, 'learning_rate': 6.625000000000001e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3141, 'grad_norm': 12.558237075805664, 'learning_rate': 6.583333333333333e-06, 'epoch': 0.73}\n",
            "{'loss': 3.2664, 'grad_norm': 10.721639633178711, 'learning_rate': 6.541666666666667e-06, 'epoch': 0.73}\n",
            "{'loss': 3.4293, 'grad_norm': 10.912728309631348, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.73}\n",
            "{'loss': 3.4348, 'grad_norm': 11.056766510009766, 'learning_rate': 6.458333333333334e-06, 'epoch': 0.73}\n",
            "{'loss': 3.4102, 'grad_norm': 10.479676246643066, 'learning_rate': 6.4166666666666665e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3404, 'grad_norm': 9.469826698303223, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.73}\n",
            "{'loss': 3.4518, 'grad_norm': 10.183850288391113, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.73}\n",
            "{'loss': 3.4496, 'grad_norm': 9.665392875671387, 'learning_rate': 6.291666666666667e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3504, 'grad_norm': 11.752543449401855, 'learning_rate': 6.25e-06, 'epoch': 0.73}\n",
            " 88% 10500/12000 [1:12:26<07:22,  3.39it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.36s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.67s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_rouge-1': 32.382577999999995, 'eval_rouge-2': 7.302537999999999, 'eval_rouge-l': 24.956189999999996, 'eval_bleu-4': 0.03538384600475767, 'eval_runtime': 68.5208, 'eval_samples_per_second': 0.73, 'eval_steps_per_second': 0.058, 'epoch': 0.73}\n",
            " 88% 10500/12000 [1:13:35<07:22,  3.39it/s]\n",
            "100% 4/4 [00:47<00:00, 11.31s/it]\u001b[A\n",
            "{'loss': 3.3365, 'grad_norm': 11.262603759765625, 'learning_rate': 6.208333333333334e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3881, 'grad_norm': 10.56519889831543, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.73}\n",
            "{'loss': 3.3467, 'grad_norm': 10.485204696655273, 'learning_rate': 6.125e-06, 'epoch': 0.74}\n",
            "{'loss': 3.2758, 'grad_norm': 11.575092315673828, 'learning_rate': 6.083333333333334e-06, 'epoch': 0.74}\n",
            "{'loss': 3.4074, 'grad_norm': 9.873675346374512, 'learning_rate': 6.041666666666667e-06, 'epoch': 0.74}\n",
            "{'loss': 3.3049, 'grad_norm': 10.137197494506836, 'learning_rate': 6e-06, 'epoch': 0.74}\n",
            "{'loss': 3.2516, 'grad_norm': 9.746012687683105, 'learning_rate': 5.958333333333334e-06, 'epoch': 0.74}\n",
            "{'loss': 3.3689, 'grad_norm': 9.708822250366211, 'learning_rate': 5.916666666666667e-06, 'epoch': 0.74}\n",
            "{'loss': 3.385, 'grad_norm': 9.544454574584961, 'learning_rate': 5.875e-06, 'epoch': 0.74}\n",
            "{'loss': 3.3365, 'grad_norm': 10.24856948852539, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.74}\n",
            "{'loss': 3.2992, 'grad_norm': 10.474451065063477, 'learning_rate': 5.7916666666666666e-06, 'epoch': 0.74}\n",
            "{'loss': 3.3236, 'grad_norm': 12.281105041503906, 'learning_rate': 5.750000000000001e-06, 'epoch': 0.74}\n",
            "{'loss': 3.3758, 'grad_norm': 9.809099197387695, 'learning_rate': 5.7083333333333335e-06, 'epoch': 0.74}\n",
            "{'loss': 3.2518, 'grad_norm': 10.336112976074219, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.74}\n",
            "{'loss': 3.3244, 'grad_norm': 9.818305015563965, 'learning_rate': 5.625e-06, 'epoch': 0.74}\n",
            "{'loss': 3.2504, 'grad_norm': 9.80698013305664, 'learning_rate': 5.583333333333334e-06, 'epoch': 0.74}\n",
            "{'loss': 3.2855, 'grad_norm': 11.005391120910645, 'learning_rate': 5.541666666666667e-06, 'epoch': 0.74}\n",
            "{'loss': 3.4072, 'grad_norm': 10.33365535736084, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.75}\n",
            "{'loss': 3.2803, 'grad_norm': 9.950881958007812, 'learning_rate': 5.458333333333333e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3387, 'grad_norm': 11.016341209411621, 'learning_rate': 5.416666666666667e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3023, 'grad_norm': 9.904687881469727, 'learning_rate': 5.375e-06, 'epoch': 0.75}\n",
            "{'loss': 3.4344, 'grad_norm': 11.285530090332031, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3248, 'grad_norm': 10.307104110717773, 'learning_rate': 5.291666666666667e-06, 'epoch': 0.75}\n",
            "{'loss': 3.4592, 'grad_norm': 9.652172088623047, 'learning_rate': 5.25e-06, 'epoch': 0.75}\n",
            "{'loss': 3.2662, 'grad_norm': 9.571853637695312, 'learning_rate': 5.208333333333334e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3447, 'grad_norm': 9.869121551513672, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.75}\n",
            "{'loss': 3.2662, 'grad_norm': 9.342156410217285, 'learning_rate': 5.125e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3908, 'grad_norm': 10.032881736755371, 'learning_rate': 5.0833333333333335e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3555, 'grad_norm': 10.286466598510742, 'learning_rate': 5.041666666666667e-06, 'epoch': 0.75}\n",
            "{'loss': 3.4746, 'grad_norm': 9.008391380310059, 'learning_rate': 5e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3273, 'grad_norm': 12.002361297607422, 'learning_rate': 4.958333333333334e-06, 'epoch': 0.75}\n",
            "{'loss': 3.3838, 'grad_norm': 10.866406440734863, 'learning_rate': 4.9166666666666665e-06, 'epoch': 0.76}\n",
            "{'loss': 3.2926, 'grad_norm': 10.513727188110352, 'learning_rate': 4.875000000000001e-06, 'epoch': 0.76}\n",
            "{'loss': 3.2395, 'grad_norm': 10.764570236206055, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3676, 'grad_norm': 11.49534797668457, 'learning_rate': 4.791666666666667e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3459, 'grad_norm': 9.976539611816406, 'learning_rate': 4.75e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3605, 'grad_norm': 9.779236793518066, 'learning_rate': 4.708333333333334e-06, 'epoch': 0.76}\n",
            "{'loss': 3.267, 'grad_norm': 9.996718406677246, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.76}\n",
            "{'loss': 3.4623, 'grad_norm': 10.845529556274414, 'learning_rate': 4.625e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3131, 'grad_norm': 9.872828483581543, 'learning_rate': 4.583333333333333e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3045, 'grad_norm': 9.872109413146973, 'learning_rate': 4.541666666666667e-06, 'epoch': 0.76}\n",
            "{'loss': 3.4111, 'grad_norm': 10.058650970458984, 'learning_rate': 4.5e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3582, 'grad_norm': 9.938271522521973, 'learning_rate': 4.4583333333333336e-06, 'epoch': 0.76}\n",
            "{'loss': 3.2908, 'grad_norm': 12.225666046142578, 'learning_rate': 4.416666666666667e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3588, 'grad_norm': 11.109380722045898, 'learning_rate': 4.375e-06, 'epoch': 0.76}\n",
            "{'loss': 3.3984, 'grad_norm': 10.467567443847656, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.77}\n",
            "{'loss': 3.2883, 'grad_norm': 10.349660873413086, 'learning_rate': 4.2916666666666665e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3648, 'grad_norm': 10.860724449157715, 'learning_rate': 4.250000000000001e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3654, 'grad_norm': 9.412433624267578, 'learning_rate': 4.208333333333333e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3746, 'grad_norm': 11.190491676330566, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.77}\n",
            " 92% 11000/12000 [1:16:12<05:01,  3.32it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.38s/it]\u001b[A\n",
            " 75% 3/4 [00:41<00:14, 14.65s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_rouge-1': 32.091167999999996, 'eval_rouge-2': 7.258808, 'eval_rouge-l': 24.131054000000002, 'eval_bleu-4': 0.03377500771901534, 'eval_runtime': 66.0758, 'eval_samples_per_second': 0.757, 'eval_steps_per_second': 0.061, 'epoch': 0.77}\n",
            " 92% 11000/12000 [1:17:18<05:01,  3.32it/s]\n",
            "100% 4/4 [00:44<00:00, 10.35s/it]\u001b[A\n",
            "{'loss': 3.2828, 'grad_norm': 10.158281326293945, 'learning_rate': 4.125e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3725, 'grad_norm': 10.396611213684082, 'learning_rate': 4.083333333333334e-06, 'epoch': 0.77}\n",
            "{'loss': 3.2816, 'grad_norm': 9.913651466369629, 'learning_rate': 4.041666666666666e-06, 'epoch': 0.77}\n",
            "{'loss': 3.4104, 'grad_norm': 9.640074729919434, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.77}\n",
            "{'loss': 3.4072, 'grad_norm': 10.333592414855957, 'learning_rate': 3.958333333333333e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3906, 'grad_norm': 11.873554229736328, 'learning_rate': 3.916666666666667e-06, 'epoch': 0.77}\n",
            "{'loss': 3.2992, 'grad_norm': 10.72150707244873, 'learning_rate': 3.875e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3188, 'grad_norm': 9.440865516662598, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3088, 'grad_norm': 11.291204452514648, 'learning_rate': 3.791666666666667e-06, 'epoch': 0.77}\n",
            "{'loss': 3.3496, 'grad_norm': 9.474773406982422, 'learning_rate': 3.75e-06, 'epoch': 0.77}\n",
            "{'loss': 3.2857, 'grad_norm': 9.880599021911621, 'learning_rate': 3.708333333333334e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3285, 'grad_norm': 10.636005401611328, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3566, 'grad_norm': 11.674863815307617, 'learning_rate': 3.625e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3598, 'grad_norm': 10.160684585571289, 'learning_rate': 3.5833333333333335e-06, 'epoch': 0.78}\n",
            "{'loss': 3.4258, 'grad_norm': 9.37752628326416, 'learning_rate': 3.541666666666667e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3322, 'grad_norm': 9.812932968139648, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3564, 'grad_norm': 11.003836631774902, 'learning_rate': 3.4583333333333334e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3352, 'grad_norm': 9.906221389770508, 'learning_rate': 3.4166666666666664e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3506, 'grad_norm': 11.092308044433594, 'learning_rate': 3.3750000000000003e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3055, 'grad_norm': 9.673470497131348, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.78}\n",
            "{'loss': 3.3871, 'grad_norm': 12.386805534362793, 'learning_rate': 3.2916666666666664e-06, 'epoch': 0.78}\n",
            "{'loss': 3.5156, 'grad_norm': 11.275331497192383, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.78}\n",
            "{'loss': 3.4016, 'grad_norm': 10.087571144104004, 'learning_rate': 3.2083333333333332e-06, 'epoch': 0.78}\n",
            "{'loss': 3.2525, 'grad_norm': 9.985027313232422, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.78}\n",
            "{'loss': 3.2377, 'grad_norm': 11.135544776916504, 'learning_rate': 3.125e-06, 'epoch': 0.79}\n",
            "{'loss': 3.3445, 'grad_norm': 10.935768127441406, 'learning_rate': 3.0833333333333336e-06, 'epoch': 0.79}\n",
            "{'loss': 3.2555, 'grad_norm': 11.344663619995117, 'learning_rate': 3.041666666666667e-06, 'epoch': 0.79}\n",
            "{'loss': 3.3156, 'grad_norm': 9.786109924316406, 'learning_rate': 3e-06, 'epoch': 0.79}\n",
            "{'loss': 3.3779, 'grad_norm': 9.797955513000488, 'learning_rate': 2.9583333333333335e-06, 'epoch': 0.79}\n",
            "{'loss': 3.2912, 'grad_norm': 10.299507141113281, 'learning_rate': 2.916666666666667e-06, 'epoch': 0.79}\n",
            "{'loss': 3.2033, 'grad_norm': 10.572515487670898, 'learning_rate': 2.8750000000000004e-06, 'epoch': 0.79}\n",
            "{'loss': 3.4188, 'grad_norm': 12.421289443969727, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.79}\n",
            "{'loss': 3.2148, 'grad_norm': 11.502084732055664, 'learning_rate': 2.791666666666667e-06, 'epoch': 0.79}\n",
            "{'loss': 3.308, 'grad_norm': 10.490798950195312, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.79}\n",
            "{'loss': 3.3814, 'grad_norm': 11.403385162353516, 'learning_rate': 2.7083333333333334e-06, 'epoch': 0.79}\n",
            "{'loss': 3.3707, 'grad_norm': 10.206006050109863, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.79}\n",
            "{'loss': 3.3605, 'grad_norm': 10.966679573059082, 'learning_rate': 2.625e-06, 'epoch': 0.79}\n",
            "{'loss': 3.443, 'grad_norm': 10.204224586486816, 'learning_rate': 2.5833333333333333e-06, 'epoch': 0.79}\n",
            "{'loss': 3.2711, 'grad_norm': 9.963258743286133, 'learning_rate': 2.5416666666666668e-06, 'epoch': 0.8}\n",
            "{'loss': 3.4449, 'grad_norm': 10.167620658874512, 'learning_rate': 2.5e-06, 'epoch': 0.8}\n",
            "{'loss': 3.2766, 'grad_norm': 10.720467567443848, 'learning_rate': 2.4583333333333332e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3369, 'grad_norm': 10.75075626373291, 'learning_rate': 2.4166666666666667e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3598, 'grad_norm': 10.672833442687988, 'learning_rate': 2.375e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3076, 'grad_norm': 9.870004653930664, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3102, 'grad_norm': 10.795071601867676, 'learning_rate': 2.2916666666666666e-06, 'epoch': 0.8}\n",
            "{'loss': 3.276, 'grad_norm': 10.263333320617676, 'learning_rate': 2.25e-06, 'epoch': 0.8}\n",
            "{'loss': 3.2887, 'grad_norm': 10.723832130432129, 'learning_rate': 2.2083333333333335e-06, 'epoch': 0.8}\n",
            "{'loss': 3.2795, 'grad_norm': 9.765345573425293, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3754, 'grad_norm': 10.331727027893066, 'learning_rate': 2.1250000000000004e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3658, 'grad_norm': 9.502840995788574, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.8}\n",
            " 96% 11500/12000 [1:19:53<02:35,  3.21it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.42s/it]\u001b[A\n",
            " 75% 3/4 [00:23<00:07,  7.34s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_rouge-1': 32.598206, 'eval_rouge-2': 7.9553780000000005, 'eval_rouge-l': 25.201397999999998, 'eval_bleu-4': 0.03917721984548753, 'eval_runtime': 32.5768, 'eval_samples_per_second': 1.535, 'eval_steps_per_second': 0.123, 'epoch': 0.8}\n",
            " 96% 11500/12000 [1:20:26<02:35,  3.21it/s]\n",
            "100% 4/4 [00:28<00:00,  6.07s/it]\u001b[A\n",
            "{'loss': 3.3988, 'grad_norm': 10.679219245910645, 'learning_rate': 2.041666666666667e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3092, 'grad_norm': 9.8098726272583, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.8}\n",
            "{'loss': 3.3252, 'grad_norm': 10.886959075927734, 'learning_rate': 1.9583333333333334e-06, 'epoch': 0.8}\n",
            "{'loss': 3.4133, 'grad_norm': 9.902183532714844, 'learning_rate': 1.916666666666667e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3543, 'grad_norm': 11.18864631652832, 'learning_rate': 1.875e-06, 'epoch': 0.81}\n",
            "{'loss': 3.4396, 'grad_norm': 9.455338478088379, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.81}\n",
            "{'loss': 3.2238, 'grad_norm': 9.907637596130371, 'learning_rate': 1.7916666666666667e-06, 'epoch': 0.81}\n",
            "{'loss': 3.4232, 'grad_norm': 11.268519401550293, 'learning_rate': 1.7500000000000002e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3123, 'grad_norm': 10.361376762390137, 'learning_rate': 1.7083333333333332e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3242, 'grad_norm': 10.024808883666992, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.81}\n",
            "{'loss': 3.2264, 'grad_norm': 10.205146789550781, 'learning_rate': 1.6250000000000001e-06, 'epoch': 0.81}\n",
            "{'loss': 3.4305, 'grad_norm': 11.039427757263184, 'learning_rate': 1.5833333333333336e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3768, 'grad_norm': 9.448858261108398, 'learning_rate': 1.5416666666666668e-06, 'epoch': 0.81}\n",
            "{'loss': 3.2754, 'grad_norm': 10.244521141052246, 'learning_rate': 1.5e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3822, 'grad_norm': 11.516342163085938, 'learning_rate': 1.4583333333333335e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3139, 'grad_norm': 10.117464065551758, 'learning_rate': 1.4166666666666667e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3471, 'grad_norm': 9.917657852172852, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.81}\n",
            "{'loss': 3.3168, 'grad_norm': 12.800555229187012, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.82}\n",
            "{'loss': 3.3455, 'grad_norm': 10.236775398254395, 'learning_rate': 1.2916666666666667e-06, 'epoch': 0.82}\n",
            "{'loss': 3.3182, 'grad_norm': 9.957944869995117, 'learning_rate': 1.25e-06, 'epoch': 0.82}\n",
            "{'loss': 3.4225, 'grad_norm': 10.107268333435059, 'learning_rate': 1.2083333333333333e-06, 'epoch': 0.82}\n",
            "{'loss': 3.3889, 'grad_norm': 9.635031700134277, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.82}\n",
            "{'loss': 3.3008, 'grad_norm': 10.350790023803711, 'learning_rate': 1.125e-06, 'epoch': 0.82}\n",
            "{'loss': 3.2443, 'grad_norm': 10.845708847045898, 'learning_rate': 1.0833333333333335e-06, 'epoch': 0.82}\n",
            "{'loss': 3.4111, 'grad_norm': 10.445759773254395, 'learning_rate': 1.0416666666666667e-06, 'epoch': 0.82}\n",
            "{'loss': 3.282, 'grad_norm': 10.287482261657715, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.82}\n",
            "{'loss': 3.3799, 'grad_norm': 12.870712280273438, 'learning_rate': 9.583333333333334e-07, 'epoch': 0.82}\n",
            "{'loss': 3.3846, 'grad_norm': 10.689837455749512, 'learning_rate': 9.166666666666667e-07, 'epoch': 0.82}\n",
            "{'loss': 3.3324, 'grad_norm': 9.76125717163086, 'learning_rate': 8.750000000000001e-07, 'epoch': 0.82}\n",
            "{'loss': 3.2719, 'grad_norm': 9.816129684448242, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.82}\n",
            "{'loss': 3.3887, 'grad_norm': 11.085411071777344, 'learning_rate': 7.916666666666668e-07, 'epoch': 0.82}\n",
            "{'loss': 3.3848, 'grad_norm': 11.593467712402344, 'learning_rate': 7.5e-07, 'epoch': 0.83}\n",
            "{'loss': 3.2764, 'grad_norm': 10.837843894958496, 'learning_rate': 7.083333333333334e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3297, 'grad_norm': 10.04766845703125, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.83}\n",
            "{'loss': 3.4098, 'grad_norm': 9.699544906616211, 'learning_rate': 6.25e-07, 'epoch': 0.83}\n",
            "{'loss': 3.4797, 'grad_norm': 11.060662269592285, 'learning_rate': 5.833333333333334e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3254, 'grad_norm': 11.174181938171387, 'learning_rate': 5.416666666666667e-07, 'epoch': 0.83}\n",
            "{'loss': 3.323, 'grad_norm': 10.813165664672852, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3004, 'grad_norm': 10.304244995117188, 'learning_rate': 4.583333333333334e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3754, 'grad_norm': 11.20809268951416, 'learning_rate': 4.1666666666666667e-07, 'epoch': 0.83}\n",
            "{'loss': 3.2787, 'grad_norm': 10.601320266723633, 'learning_rate': 3.75e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3238, 'grad_norm': 10.308788299560547, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.83}\n",
            "{'loss': 3.2021, 'grad_norm': 9.734745025634766, 'learning_rate': 2.916666666666667e-07, 'epoch': 0.83}\n",
            "{'loss': 3.4178, 'grad_norm': 9.674596786499023, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3455, 'grad_norm': 11.173691749572754, 'learning_rate': 2.0833333333333333e-07, 'epoch': 0.83}\n",
            "{'loss': 3.2662, 'grad_norm': 10.495067596435547, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.83}\n",
            "{'loss': 3.3348, 'grad_norm': 9.734658241271973, 'learning_rate': 1.2500000000000002e-07, 'epoch': 0.84}\n",
            "{'loss': 3.3102, 'grad_norm': 9.149580001831055, 'learning_rate': 8.333333333333334e-08, 'epoch': 0.84}\n",
            "{'loss': 3.268, 'grad_norm': 10.620146751403809, 'learning_rate': 4.166666666666667e-08, 'epoch': 0.84}\n",
            "{'loss': 3.3764, 'grad_norm': 10.934412956237793, 'learning_rate': 0.0, 'epoch': 0.84}\n",
            "100% 12000/12000 [1:23:02<00:00,  3.04it/s]***** Running Evaluation *****\n",
            "  Num examples = 50\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:20<00:20, 10.42s/it]\u001b[A\n",
            " 75% 3/4 [00:24<00:07,  7.50s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_rouge-1': 32.907740000000004, 'eval_rouge-2': 7.422116, 'eval_rouge-l': 24.563544, 'eval_bleu-4': 0.03641288780736966, 'eval_runtime': 31.6843, 'eval_samples_per_second': 1.578, 'eval_steps_per_second': 0.126, 'epoch': 0.84}\n",
            "100% 12000/12000 [1:23:34<00:00,  3.04it/s]\n",
            "100% 4/4 [00:27<00:00,  5.85s/it]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-12000\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
            "Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 8192,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5015.5757, 'train_samples_per_second': 19.14, 'train_steps_per_second': 2.393, 'train_loss': 3.40302587890625, 'epoch': 0.84}\n",
            "100% 12000/12000 [1:23:35<00:00,  2.39it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "***** Running Prediction *****\n",
            "  Num examples = 1070\n",
            "  Batch size = 16\n",
            "100% 67/67 [13:33<00:00, 12.14s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_steps: 12000\n",
        "# per_device_train_batch_size: 8"
      ],
      "metadata": {
        "id": "k9rg5sGBxkk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}